{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Aggregation Backup: Interactive Demo\n",
    "\n",
    "This notebook demonstrates the **variance aggregation backup** mechanism in Bayesian BAI-MCTS.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Aggregated Beliefs**: Parent nodes maintain beliefs about their best child using optimality weights\n",
    "2. **Optimality Weights**: Probability each child is optimal via pairwise Gaussian CDF\n",
    "3. **Variance Propagation**: `agg_sigma_sq` from children becomes observation variance for parents\n",
    "4. **Ensemble Effect**: Variance decreases as 1/√N due to squared weight aggregation\n",
    "\n",
    "**Setup:** Use `Runtime > Change runtime type > GPU` for faster model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/caldred/nanozero.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd nanozero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import from nanozero\n",
    "from nanozero.bayesian_mcts import BayesianNode, BayesianMCTS, normal_cdf\n",
    "from nanozero.config import BayesianMCTSConfig\n",
    "from nanozero.game import get_game\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: BayesianNode Basics\n",
    "\n",
    "A `BayesianNode` maintains:\n",
    "- `mu`, `sigma_sq`: Gaussian belief about this node's value\n",
    "- `agg_mu`, `agg_sigma_sq`: Aggregated belief from children (expected value of best child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node(node, name=\"Node\", indent=0):\n",
    "    \"\"\"Pretty print a BayesianNode.\"\"\"\n",
    "    prefix = \"  \" * indent\n",
    "    print(f\"{prefix}{name}:\")\n",
    "    print(f\"{prefix}  belief: μ={node.mu:.4f}, σ²={node.sigma_sq:.4f} (σ={math.sqrt(node.sigma_sq):.4f})\")\n",
    "    if node.agg_mu is not None:\n",
    "        print(f\"{prefix}  aggregated: μ_agg={node.agg_mu:.4f}, σ²_agg={node.agg_sigma_sq:.4f}\")\n",
    "    print(f\"{prefix}  precision: {node.precision():.2f}\")\n",
    "\n",
    "# Create a simple node\n",
    "node = BayesianNode(prior=0.3, mu=0.5, sigma_sq=0.25)\n",
    "print_node(node, \"Initial Node\")\n",
    "\n",
    "# Update with an observation\n",
    "print(\"\\n--- Updating with observation value=0.8, obs_var=0.1 ---\\n\")\n",
    "node.update(value=0.8, obs_var=0.1)\n",
    "print_node(node, \"After Update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Aggregation with Multiple Children\n",
    "\n",
    "When a node has children, `aggregate_children()` computes:\n",
    "1. **Optimality scores** via pairwise Gaussian CDF comparisons\n",
    "2. **Weights** = normalized scores (soft-pruned)\n",
    "3. **Aggregated mean** = weighted average of child means\n",
    "4. **Aggregated variance** = `Σ w²[σ² + (μ - μ_agg)²]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree_with_children(child_beliefs):\n",
    "    \"\"\"\n",
    "    Create a parent node with children having specified beliefs.\n",
    "    \n",
    "    child_beliefs: list of (mu, sigma_sq) tuples\n",
    "    \"\"\"\n",
    "    parent = BayesianNode()\n",
    "    for i, (mu, sigma_sq) in enumerate(child_beliefs):\n",
    "        parent.children[i] = BayesianNode(prior=1.0/len(child_beliefs), mu=mu, sigma_sq=sigma_sq)\n",
    "    return parent\n",
    "\n",
    "# Scenario 1: Clear winner (child 0 much better)\n",
    "print(\"=\" * 60)\n",
    "print(\"Scenario 1: Clear Winner\")\n",
    "print(\"=\" * 60)\n",
    "# Note: children store values from child's perspective (opponent)\n",
    "# So a child with mu=-0.8 means the parent expects value +0.8 from that action\n",
    "parent1 = create_tree_with_children([\n",
    "    (-0.8, 0.04),   # Child 0: looks great for parent (mu=+0.8 from parent view)\n",
    "    (-0.2, 0.04),   # Child 1: mediocre\n",
    "    (-0.1, 0.04),   # Child 2: mediocre\n",
    "])\n",
    "\n",
    "print(\"\\nBefore aggregation:\")\n",
    "for a, c in parent1.children.items():\n",
    "    print(f\"  Child {a}: μ={c.mu:.3f}, σ²={c.sigma_sq:.3f} → parent view: {-c.mu:.3f}\")\n",
    "\n",
    "parent1.aggregate_children(prune_threshold=0.01)\n",
    "print(f\"\\nAfter aggregation:\")\n",
    "print(f\"  agg_mu={parent1.agg_mu:.4f} (expected value of best child)\")\n",
    "print(f\"  agg_sigma_sq={parent1.agg_sigma_sq:.6f} (uncertainty about best)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Uncertain (children have similar means but high variance)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Scenario 2: High Uncertainty\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "parent2 = create_tree_with_children([\n",
    "    (-0.4, 0.25),   # High variance\n",
    "    (-0.35, 0.25),\n",
    "    (-0.3, 0.25),\n",
    "])\n",
    "\n",
    "print(\"\\nBefore aggregation (high variance children):\")\n",
    "for a, c in parent2.children.items():\n",
    "    print(f\"  Child {a}: μ={c.mu:.3f}, σ²={c.sigma_sq:.3f}\")\n",
    "\n",
    "parent2.aggregate_children(prune_threshold=0.01)\n",
    "print(f\"\\nAfter aggregation:\")\n",
    "print(f\"  agg_mu={parent2.agg_mu:.4f}\")\n",
    "print(f\"  agg_sigma_sq={parent2.agg_sigma_sq:.6f} (higher than Scenario 1!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Visualize optimality weights\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Scenario 3: Optimality Weights Visualization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compute_optimality_weights(child_beliefs, prune_threshold=0.01):\n",
    "    \"\"\"Compute optimality weights for visualization.\"\"\"\n",
    "    # Get beliefs from parent's perspective\n",
    "    mus = np.array([-mu for mu, _ in child_beliefs])\n",
    "    sigma_sqs = np.array([s for _, s in child_beliefs])\n",
    "    n = len(child_beliefs)\n",
    "    \n",
    "    sorted_idx = np.argsort(mus)[::-1]\n",
    "    leader_idx = sorted_idx[0]\n",
    "    challenger_idx = sorted_idx[1]\n",
    "    \n",
    "    scores = np.zeros(n)\n",
    "    mu_L, sigma_sq_L = mus[leader_idx], sigma_sqs[leader_idx]\n",
    "    mu_C, sigma_sq_C = mus[challenger_idx], sigma_sqs[challenger_idx]\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == leader_idx:\n",
    "            diff = mu_L - mu_C\n",
    "            std = math.sqrt(sigma_sq_L + sigma_sq_C)\n",
    "        else:\n",
    "            diff = mus[i] - mu_L\n",
    "            std = math.sqrt(sigma_sqs[i] + sigma_sq_L)\n",
    "        scores[i] = normal_cdf(diff / std) if std > 1e-10 else (1.0 if diff > 0 else 0.0)\n",
    "    \n",
    "    scores[scores < prune_threshold] = 0.0\n",
    "    weights = scores / (scores.sum() + 1e-10)\n",
    "    return mus, scores, weights\n",
    "\n",
    "# Different scenarios\n",
    "scenarios = {\n",
    "    \"Clear Winner\": [(-0.8, 0.04), (-0.2, 0.04), (-0.1, 0.04)],\n",
    "    \"Close Race\": [(-0.52, 0.04), (-0.50, 0.04), (-0.48, 0.04)],\n",
    "    \"High Variance\": [(-0.6, 0.25), (-0.5, 0.25), (-0.4, 0.25)],\n",
    "    \"Mixed\": [(-0.7, 0.01), (-0.5, 0.16), (-0.3, 0.04)],\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (name, beliefs) in zip(axes, scenarios.items()):\n",
    "    mus, scores, weights = compute_optimality_weights(beliefs)\n",
    "    \n",
    "    x = np.arange(len(beliefs))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, scores, width, label='Optimality Score', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, weights, width, label='Normalized Weight', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Action')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'{name}\\n(μ from parent view: {[f\"{m:.2f}\" for m in mus]})')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'Action {i}' for i in range(len(beliefs))])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Backup with Variance Propagation\n",
    "\n",
    "The key insight: **`agg_sigma_sq` from children becomes `obs_var` for parents.**\n",
    "\n",
    "This means:\n",
    "- Uncertain subtrees contribute observations with higher variance\n",
    "- Confident subtrees contribute observations with lower variance\n",
    "- Parent precision reflects true epistemic uncertainty about the best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_backup_chain(depths=3, children_per_node=3, n_backups=20):\n",
    "    \"\"\"\n",
    "    Simulate backup chain and track variance propagation.\n",
    "    \n",
    "    Creates a linear chain of nodes (like a search path) and\n",
    "    repeatedly backs up values, tracking how variance changes.\n",
    "    \"\"\"\n",
    "    sigma_0 = 0.5\n",
    "    base_obs_var = 0.25\n",
    "    \n",
    "    # Create chain of nodes (root -> level1 -> level2 -> leaf)\n",
    "    nodes = []\n",
    "    for d in range(depths):\n",
    "        node = BayesianNode(mu=0.0, sigma_sq=sigma_0**2)\n",
    "        # Add children\n",
    "        for i in range(children_per_node):\n",
    "            child_mu = np.random.normal(0, 0.3)\n",
    "            node.children[i] = BayesianNode(mu=child_mu, sigma_sq=sigma_0**2)\n",
    "        node.aggregate_children()\n",
    "        nodes.append(node)\n",
    "    \n",
    "    # Track metrics over backups\n",
    "    history = {\n",
    "        'backup': [],\n",
    "        'root_agg_mu': [],\n",
    "        'root_agg_sigma_sq': [],\n",
    "        'level1_agg_sigma_sq': [],\n",
    "        'obs_var_at_root': [],\n",
    "    }\n",
    "    \n",
    "    for backup_num in range(n_backups):\n",
    "        # Simulate a value coming from the leaf\n",
    "        leaf_value = np.random.normal(0.5, 0.3)  # Pretend leaf says +0.5 ish\n",
    "        \n",
    "        # Backup through the chain\n",
    "        value = leaf_value\n",
    "        obs_var = base_obs_var\n",
    "        \n",
    "        for d in range(depths - 1, -1, -1):\n",
    "            node = nodes[d]\n",
    "            # Update a random child (simulating MCTS path)\n",
    "            action = np.random.randint(0, children_per_node)\n",
    "            node.children[action].update(value, obs_var, min_var=1e-6)\n",
    "            \n",
    "            # Recompute aggregation\n",
    "            node.aggregate_children()\n",
    "            \n",
    "            # Propagate\n",
    "            value = -node.agg_mu\n",
    "            obs_var = node.agg_sigma_sq\n",
    "        \n",
    "        # Record\n",
    "        history['backup'].append(backup_num)\n",
    "        history['root_agg_mu'].append(nodes[0].agg_mu)\n",
    "        history['root_agg_sigma_sq'].append(nodes[0].agg_sigma_sq)\n",
    "        if depths > 1:\n",
    "            history['level1_agg_sigma_sq'].append(nodes[1].agg_sigma_sq)\n",
    "        history['obs_var_at_root'].append(obs_var)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run simulation\n",
    "np.random.seed(42)\n",
    "history = simulate_backup_chain(depths=3, children_per_node=3, n_backups=50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Root aggregated mean\n",
    "axes[0].plot(history['backup'], history['root_agg_mu'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Backup #')\n",
    "axes[0].set_ylabel('Root agg_mu')\n",
    "axes[0].set_title('Root Aggregated Mean Over Backups')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Aggregated variance at different levels\n",
    "axes[1].plot(history['backup'], history['root_agg_sigma_sq'], 'b-', linewidth=2, label='Root')\n",
    "axes[1].plot(history['backup'], history['level1_agg_sigma_sq'], 'r--', linewidth=2, label='Level 1')\n",
    "axes[1].set_xlabel('Backup #')\n",
    "axes[1].set_ylabel('agg_sigma_sq')\n",
    "axes[1].set_title('Aggregated Variance at Different Levels')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Observation variance propagated to root\n",
    "axes[2].plot(history['backup'], history['obs_var_at_root'], 'g-', linewidth=2)\n",
    "axes[2].axhline(y=0.25, color='gray', linestyle='--', label='Initial obs_var')\n",
    "axes[2].set_xlabel('Backup #')\n",
    "axes[2].set_ylabel('obs_var at root level')\n",
    "axes[2].set_title('Propagated Observation Variance')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal root agg_sigma_sq: {history['root_agg_sigma_sq'][-1]:.6f}\")\n",
    "print(f\"Initial root agg_sigma_sq: {history['root_agg_sigma_sq'][0]:.6f}\")\n",
    "print(f\"Reduction factor: {history['root_agg_sigma_sq'][0] / history['root_agg_sigma_sq'][-1]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Full MCTS Search Visualization\n",
    "\n",
    "Let's run actual MCTS on TicTacToe and visualize the beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load game and create a simple model\n",
    "game = get_game('tictactoe')\n",
    "print(f\"Game: {game.config.name}\")\n",
    "print(f\"Action size: {game.config.action_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanozero.model import AlphaZeroTransformer\n",
    "from nanozero.config import get_model_config\n",
    "\n",
    "# Create a small model (untrained, just for demo)\n",
    "model_config = get_model_config(game.config, n_layer=2)\n",
    "model = AlphaZeroTransformer(model_config).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BayesianMCTS\n",
    "config = BayesianMCTSConfig(\n",
    "    num_simulations=100,\n",
    "    sigma_0=0.5,\n",
    "    obs_var=0.25,\n",
    "    prune_threshold=0.01,\n",
    "    early_stopping=False,  # Disable for visualization\n",
    ")\n",
    "\n",
    "mcts = BayesianMCTS(game, config)\n",
    "print(\"BayesianMCTS created with variance aggregation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_mcts_tree(root, game, max_depth=2, title=\"MCTS Tree\"):\n    \"\"\"Visualize the MCTS tree with beliefs.\"\"\"\n    \n    def format_belief(node):\n        agg_str = \"\"\n        if node.agg_mu is not None:\n            agg_str = f\"\\nagg: μ={node.agg_mu:.2f}, σ²={node.agg_sigma_sq:.3f}\"\n        return f\"μ={node.mu:.2f}, σ²={node.sigma_sq:.3f}{agg_str}\"\n    \n    def print_tree(node, depth=0, action=None, prefix=\"\"):\n        if depth > max_depth:\n            return\n        \n        indent = \"  \" * depth\n        action_str = f\"Action {action}: \" if action is not None else \"\"\n        print(f\"{indent}{prefix}{action_str}{format_belief(node)}\")\n        \n        for a, child in sorted(node.children.items()):\n            print_tree(child, depth + 1, a, \"└─ \")\n    \n    print(f\"\\n{'='*60}\")\n    print(title)\n    print(\"='*60\")\n    print_tree(root)\n\n# Run search on empty board\nstate = game.initial_state()\nprint(\"TicTacToe board (X to move):\")\nprint(game.display(state))\n\n# Manually expand root and run a few simulations\nroot = BayesianNode()\nmcts._expand(root, state, model, device)\n\nprint(f\"\\nAfter expansion (before simulations):\")\nprint(f\"Root aggregated belief: μ={root.agg_mu:.4f}, σ²={root.agg_sigma_sq:.6f}\")\nprint(f\"\\nChildren beliefs (from root's perspective):\")\nfor a, c in sorted(root.children.items()):\n    parent_view_mu = -c.mu\n    print(f\"  Action {a}: μ={c.mu:.3f} (parent sees: {parent_view_mu:.3f}), σ²={c.sigma_sq:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulations and track convergence\n",
    "state = game.initial_state()\n",
    "root = BayesianNode()\n",
    "\n",
    "# Expand root\n",
    "mcts._expand(root, state, model, device)\n",
    "\n",
    "# Track metrics\n",
    "sim_history = {\n",
    "    'sim': [],\n",
    "    'agg_mu': [],\n",
    "    'agg_sigma_sq': [],\n",
    "    'child_sigma_sqs': {a: [] for a in root.children.keys()},\n",
    "}\n",
    "\n",
    "# Record initial state\n",
    "sim_history['sim'].append(0)\n",
    "sim_history['agg_mu'].append(root.agg_mu)\n",
    "sim_history['agg_sigma_sq'].append(root.agg_sigma_sq)\n",
    "for a, c in root.children.items():\n",
    "    sim_history['child_sigma_sqs'][a].append(c.sigma_sq)\n",
    "\n",
    "# Run simulations\n",
    "for sim in range(1, 101):\n",
    "    mcts._run_simulation(root, state, model, device)\n",
    "    \n",
    "    sim_history['sim'].append(sim)\n",
    "    sim_history['agg_mu'].append(root.agg_mu)\n",
    "    sim_history['agg_sigma_sq'].append(root.agg_sigma_sq)\n",
    "    for a, c in root.children.items():\n",
    "        sim_history['child_sigma_sqs'][a].append(c.sigma_sq)\n",
    "\n",
    "print(f\"Ran 100 simulations.\")\n",
    "print(f\"Final root agg_sigma_sq: {sim_history['agg_sigma_sq'][-1]:.6f}\")\n",
    "print(f\"Initial root agg_sigma_sq: {sim_history['agg_sigma_sq'][0]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Root aggregated mean\n",
    "axes[0].plot(sim_history['sim'], sim_history['agg_mu'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Simulation #')\n",
    "axes[0].set_ylabel('Root agg_mu')\n",
    "axes[0].set_title('Root Value Estimate')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Root aggregated variance\n",
    "axes[1].plot(sim_history['sim'], sim_history['agg_sigma_sq'], 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('Simulation #')\n",
    "axes[1].set_ylabel('Root agg_sigma_sq')\n",
    "axes[1].set_title('Root Variance (Uncertainty)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Child variances\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 9))\n",
    "for i, (a, variances) in enumerate(sim_history['child_sigma_sqs'].items()):\n",
    "    axes[2].plot(sim_history['sim'], variances, color=colors[a], \n",
    "                 linewidth=1.5, alpha=0.7, label=f'Action {a}')\n",
    "axes[2].set_xlabel('Simulation #')\n",
    "axes[2].set_ylabel('Child sigma_sq')\n",
    "axes[2].set_title('Child Variances')\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final policy\n",
    "policy = mcts._get_policy(root)\n",
    "\n",
    "print(\"\\nFinal Policy (probability each action is optimal):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display as 3x3 grid\n",
    "print(\"\\nTicTacToe grid (policy values):\")\n",
    "for row in range(3):\n",
    "    row_str = \"  \"\n",
    "    for col in range(3):\n",
    "        action = row * 3 + col\n",
    "        row_str += f\" {policy[action]:.2f} \"\n",
    "    print(row_str)\n",
    "\n",
    "print(f\"\\nBest action: {np.argmax(policy)} (probability: {np.max(policy):.3f})\")\n",
    "\n",
    "# Show child beliefs\n",
    "print(\"\\nChild beliefs after search:\")\n",
    "for a in sorted(root.children.keys()):\n",
    "    c = root.children[a]\n",
    "    parent_view = -c.mu\n",
    "    print(f\"  Action {a}: value={parent_view:.3f}, variance={c.sigma_sq:.4f}, policy={policy[a]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Comparing Prune Thresholds\n",
    "\n",
    "The `prune_threshold` parameter controls soft-pruning of low-probability children.\n",
    "Higher threshold = more aggressive pruning = sharper policies but potentially more bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search_with_threshold(threshold, n_sims=50):\n",
    "    \"\"\"Run MCTS with a specific prune threshold.\"\"\"\n",
    "    config = BayesianMCTSConfig(\n",
    "        num_simulations=n_sims,\n",
    "        prune_threshold=threshold,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    local_mcts = BayesianMCTS(game, config)\n",
    "    \n",
    "    state = game.initial_state()\n",
    "    policy = local_mcts.search(state[np.newaxis, ...], model)[0]\n",
    "    \n",
    "    return policy\n",
    "\n",
    "thresholds = [0.0, 0.01, 0.05, 0.1, 0.2]\n",
    "policies_by_threshold = {}\n",
    "\n",
    "for thresh in thresholds:\n",
    "    policies_by_threshold[thresh] = run_search_with_threshold(thresh)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, len(thresholds), figsize=(15, 3))\n",
    "\n",
    "for ax, thresh in zip(axes, thresholds):\n",
    "    policy = policies_by_threshold[thresh]\n",
    "    im = ax.imshow(policy.reshape(3, 3), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'threshold={thresh}')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Annotate with values\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            ax.text(j, i, f'{policy[i*3+j]:.2f}', ha='center', va='center', \n",
    "                   color='white' if policy[i*3+j] > 0.5 else 'black', fontsize=10)\n",
    "\n",
    "plt.suptitle('Policy Heatmaps by Prune Threshold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Entropy comparison\n",
    "print(\"\\nPolicy entropy by threshold:\")\n",
    "for thresh in thresholds:\n",
    "    policy = policies_by_threshold[thresh]\n",
    "    ent = -np.sum(policy * np.log(policy + 1e-8))\n",
    "    print(f\"  threshold={thresh}: entropy={ent:.3f}, max_prob={np.max(policy):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Interactive Exploration\n",
    "\n",
    "Play with different parameters and see how they affect the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive parameter exploration\n",
    "def explore_parameters(sigma_0=0.5, obs_var=0.25, prune_threshold=0.01, n_sims=50):\n",
    "    \"\"\"Run search with custom parameters and display results.\"\"\"\n",
    "    config = BayesianMCTSConfig(\n",
    "        num_simulations=n_sims,\n",
    "        sigma_0=sigma_0,\n",
    "        obs_var=obs_var,\n",
    "        prune_threshold=prune_threshold,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    local_mcts = BayesianMCTS(game, config)\n",
    "    \n",
    "    state = game.initial_state()\n",
    "    \n",
    "    # Manual search to access root\n",
    "    root = BayesianNode()\n",
    "    local_mcts._expand(root, state, model, device)\n",
    "    \n",
    "    for _ in range(n_sims):\n",
    "        local_mcts._run_simulation(root, state, model, device)\n",
    "    \n",
    "    policy = local_mcts._get_policy(root)\n",
    "    \n",
    "    print(f\"Parameters: sigma_0={sigma_0}, obs_var={obs_var}, prune_threshold={prune_threshold}, n_sims={n_sims}\")\n",
    "    print(f\"Root aggregated: μ={root.agg_mu:.4f}, σ²={root.agg_sigma_sq:.6f}\")\n",
    "    print(f\"Policy entropy: {-np.sum(policy * np.log(policy + 1e-8)):.3f}\")\n",
    "    print(f\"Best action: {np.argmax(policy)} (prob={np.max(policy):.3f})\")\n",
    "    print(\"\\nPolicy grid:\")\n",
    "    for row in range(3):\n",
    "        print(\"  \" + \" \".join(f\"{policy[row*3+col]:.2f}\" for col in range(3)))\n",
    "    \n",
    "    return root, policy\n",
    "\n",
    "# Try different configurations\n",
    "print(\"=\"*60)\n",
    "print(\"Low prior variance (sigma_0=0.1):\")\n",
    "print(\"=\"*60)\n",
    "explore_parameters(sigma_0=0.1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"High prior variance (sigma_0=1.0):\")\n",
    "print(\"=\"*60)\n",
    "explore_parameters(sigma_0=1.0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Low observation variance (obs_var=0.05):\")\n",
    "print(\"=\"*60)\n",
    "explore_parameters(obs_var=0.05)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"More simulations (n_sims=200):\")\n",
    "print(\"=\"*60)\n",
    "explore_parameters(n_sims=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Variance Aggregation** computes parent beliefs from ALL children, not just visited ones\n",
    "\n",
    "2. **Optimality Weights** use pairwise Gaussian CDF: `P(child > leader)` for each child\n",
    "\n",
    "3. **Variance Formula**: `σ²_agg = Σ w²[σ² + (μ - μ_agg)²]`\n",
    "   - Squared weights → ensemble effect (variance decreases as 1/√N)\n",
    "   - Disagreement term → uncertain when children disagree\n",
    "\n",
    "4. **Variance Propagation**: `agg_sigma_sq` becomes `obs_var` for parent level\n",
    "   - Uncertain subtrees contribute high-variance observations\n",
    "   - Confident subtrees contribute low-variance observations\n",
    "\n",
    "5. **Policy Extraction** uses optimality weights directly (deterministic, fast)\n",
    "\n",
    "### Parameters That Matter\n",
    "\n",
    "| Parameter | Effect |\n",
    "|-----------|--------|\n",
    "| `sigma_0` | Prior uncertainty - higher = slower convergence but more exploration |\n",
    "| `obs_var` | Leaf observation noise - higher = slower belief updates |\n",
    "| `prune_threshold` | Soft-prune weak children - higher = sharper policies |\n",
    "| `n_sims` | More simulations = lower variance = more confident policies |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete! Feel free to modify and re-run cells to explore.\")"
   ]
  }
 ]
}