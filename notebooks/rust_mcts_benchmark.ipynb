{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rust MCTS Benchmark\n",
    "\n",
    "This notebook benchmarks the Rust MCTS backend vs Python MCTS on a GPU.\n",
    "\n",
    "With a powerful GPU, neural network inference is fast, so we can see the true\n",
    "speedup from Rust tree operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/caldred/nanozero.git\n",
    "%cd nanozero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Rust toolchain\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ['PATH'] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
    "!rustc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python dependencies\n",
    "!pip install -q torch numpy maturin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Rust MCTS backend\n",
    "%cd nanozero-mcts-rs\n",
    "!maturin develop --release\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "from nanozero_mcts_rs import RustBatchedMCTS\n",
    "print(\"Rust MCTS backend loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Game and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from nanozero.game import get_game\n",
    "from nanozero.model import AlphaZeroTransformer\n",
    "from nanozero.mcts import RustBatchedMCTS, BatchedMCTS\n",
    "from nanozero.config import get_model_config, MCTSConfig\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TicTacToe game and model\n",
    "game = get_game('tictactoe')\n",
    "model_config = get_model_config(game.config, n_layer=4)  # Larger model\n",
    "model = AlphaZeroTransformer(model_config).to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Game: TicTacToe\")\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark: Varying Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mcts(game, model, batch_sizes, num_simulations=100, n_runs=5):\n",
    "    \"\"\"Benchmark Rust vs Python MCTS across different batch sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        config = MCTSConfig(num_simulations=num_simulations)\n",
    "        \n",
    "        rust_mcts = RustBatchedMCTS(game, config)\n",
    "        python_mcts = BatchedMCTS(game, config, use_transposition_table=False)\n",
    "        \n",
    "        # Create batch of initial states\n",
    "        state = game.initial_state()\n",
    "        states = np.stack([state] * batch_size)\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.inference_mode():\n",
    "            _ = rust_mcts.search(states, model, add_noise=False)\n",
    "            _ = python_mcts.search(states, model, add_noise=False)\n",
    "        \n",
    "        # Synchronize GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark Rust\n",
    "        rust_times = []\n",
    "        for _ in range(n_runs):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                _ = rust_mcts.search(states, model, add_noise=False)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            rust_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        # Benchmark Python\n",
    "        python_times = []\n",
    "        for _ in range(n_runs):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                _ = python_mcts.search(states, model, add_noise=False)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            python_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        rust_avg = np.mean(rust_times) * 1000\n",
    "        python_avg = np.mean(python_times) * 1000\n",
    "        speedup = python_avg / rust_avg\n",
    "        \n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'rust_ms': rust_avg,\n",
    "            'python_ms': python_avg,\n",
    "            'speedup': speedup\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch {batch_size:3d}: Rust={rust_avg:7.1f}ms, Python={python_avg:7.1f}ms, Speedup={speedup:.2f}x\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Benchmark: Varying Batch Size (100 simulations)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_results = benchmark_mcts(\n",
    "    game, model,\n",
    "    batch_sizes=[8, 16, 32, 64, 128, 256],\n",
    "    num_simulations=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark: Varying Simulation Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_simulations(game, model, batch_size, sim_counts, n_runs=3):\n",
    "    \"\"\"Benchmark Rust vs Python MCTS across different simulation counts.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for num_sims in sim_counts:\n",
    "        config = MCTSConfig(num_simulations=num_sims)\n",
    "        \n",
    "        rust_mcts = RustBatchedMCTS(game, config)\n",
    "        python_mcts = BatchedMCTS(game, config, use_transposition_table=False)\n",
    "        \n",
    "        state = game.initial_state()\n",
    "        states = np.stack([state] * batch_size)\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.inference_mode():\n",
    "            _ = rust_mcts.search(states, model, add_noise=False)\n",
    "            _ = python_mcts.search(states, model, add_noise=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        rust_times = []\n",
    "        for _ in range(n_runs):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                _ = rust_mcts.search(states, model, add_noise=False)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            rust_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        python_times = []\n",
    "        for _ in range(n_runs):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            with torch.inference_mode():\n",
    "                _ = python_mcts.search(states, model, add_noise=False)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            python_times.append(time.perf_counter() - start)\n",
    "        \n",
    "        rust_avg = np.mean(rust_times) * 1000\n",
    "        python_avg = np.mean(python_times) * 1000\n",
    "        speedup = python_avg / rust_avg\n",
    "        \n",
    "        results.append({\n",
    "            'num_sims': num_sims,\n",
    "            'rust_ms': rust_avg,\n",
    "            'python_ms': python_avg,\n",
    "            'speedup': speedup\n",
    "        })\n",
    "        \n",
    "        print(f\"{num_sims:4d} sims: Rust={rust_avg:7.1f}ms, Python={python_avg:7.1f}ms, Speedup={speedup:.2f}x\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Benchmark: Varying Simulations (batch_size=64)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sim_results = benchmark_simulations(\n",
    "    game, model,\n",
    "    batch_size=64,\n",
    "    sim_counts=[25, 50, 100, 200, 400, 800]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark: Connect4 (Larger Game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Connect4 game and model\n",
    "game_c4 = get_game('connect4')\n",
    "model_config_c4 = get_model_config(game_c4.config, n_layer=4)\n",
    "model_c4 = AlphaZeroTransformer(model_config_c4).to(device)\n",
    "model_c4.eval()\n",
    "\n",
    "print(f\"Game: Connect4\")\n",
    "print(f\"Board size: {game_c4.config.board_size}\")\n",
    "print(f\"Action size: {game_c4.config.action_size}\")\n",
    "print(f\"Model parameters: {model_c4.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Benchmark: Connect4 - Varying Batch Size (100 sims)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "c4_results = benchmark_mcts(\n",
    "    game_c4, model_c4,\n",
    "    batch_sizes=[8, 16, 32, 64, 128],\n",
    "    num_simulations=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Throughput Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_throughput(game, model, batch_size, num_simulations, duration_seconds=10):\n",
    "    \"\"\"Measure searches per second for both backends.\"\"\"\n",
    "    config = MCTSConfig(num_simulations=num_simulations)\n",
    "    \n",
    "    rust_mcts = RustBatchedMCTS(game, config)\n",
    "    python_mcts = BatchedMCTS(game, config, use_transposition_table=False)\n",
    "    \n",
    "    state = game.initial_state()\n",
    "    states = np.stack([state] * batch_size)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = rust_mcts.search(states, model, add_noise=False)\n",
    "        _ = python_mcts.search(states, model, add_noise=False)\n",
    "    \n",
    "    # Measure Rust throughput\n",
    "    rust_count = 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    while time.perf_counter() - start < duration_seconds:\n",
    "        with torch.inference_mode():\n",
    "            _ = rust_mcts.search(states, model, add_noise=False)\n",
    "        rust_count += batch_size\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    rust_elapsed = time.perf_counter() - start\n",
    "    rust_throughput = rust_count / rust_elapsed\n",
    "    \n",
    "    # Measure Python throughput\n",
    "    python_count = 0\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    while time.perf_counter() - start < duration_seconds:\n",
    "        with torch.inference_mode():\n",
    "            _ = python_mcts.search(states, model, add_noise=False)\n",
    "        python_count += batch_size\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    python_elapsed = time.perf_counter() - start\n",
    "    python_throughput = python_count / python_elapsed\n",
    "    \n",
    "    return rust_throughput, python_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Throughput Test (searches/second)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for game_name, g, m in [('TicTacToe', game, model), ('Connect4', game_c4, model_c4)]:\n",
    "    print(f\"\\n{game_name}:\")\n",
    "    for batch_size in [32, 64, 128]:\n",
    "        rust_tp, python_tp = measure_throughput(g, m, batch_size, num_simulations=100, duration_seconds=5)\n",
    "        print(f\"  Batch {batch_size:3d}: Rust={rust_tp:6.1f}/s, Python={python_tp:6.1f}/s, Speedup={rust_tp/python_tp:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot batch size results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "ax1 = axes[0]\n",
    "batch_sizes = [r['batch_size'] for r in batch_results]\n",
    "rust_times = [r['rust_ms'] for r in batch_results]\n",
    "python_times = [r['python_ms'] for r in batch_results]\n",
    "\n",
    "x = np.arange(len(batch_sizes))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, rust_times, width, label='Rust MCTS', color='#E67E22')\n",
    "ax1.bar(x + width/2, python_times, width, label='Python MCTS', color='#3498DB')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Time (ms)')\n",
    "ax1.set_title('MCTS Search Time by Batch Size (100 sims)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(batch_sizes)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "ax2 = axes[1]\n",
    "speedups = [r['speedup'] for r in batch_results]\n",
    "colors = ['#2ECC71' if s >= 1 else '#E74C3C' for s in speedups]\n",
    "ax2.bar(x, speedups, color=colors)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Speedup (Rust/Python)')\n",
    "ax2.set_title('Rust MCTS Speedup vs Python')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(batch_sizes)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation count results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "ax1 = axes[0]\n",
    "sim_counts = [r['num_sims'] for r in sim_results]\n",
    "rust_times = [r['rust_ms'] for r in sim_results]\n",
    "python_times = [r['python_ms'] for r in sim_results]\n",
    "\n",
    "ax1.plot(sim_counts, rust_times, 'o-', label='Rust MCTS', color='#E67E22', linewidth=2)\n",
    "ax1.plot(sim_counts, python_times, 's-', label='Python MCTS', color='#3498DB', linewidth=2)\n",
    "ax1.set_xlabel('Number of Simulations')\n",
    "ax1.set_ylabel('Time (ms)')\n",
    "ax1.set_title('MCTS Search Time by Simulation Count (batch=64)')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "ax2 = axes[1]\n",
    "speedups = [r['speedup'] for r in sim_results]\n",
    "ax2.plot(sim_counts, speedups, 'o-', color='#9B59B6', linewidth=2)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Number of Simulations')\n",
    "ax2.set_ylabel('Speedup (Rust/Python)')\n",
    "ax2.set_title('Rust MCTS Speedup vs Simulation Count')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.fill_between(sim_counts, speedups, 1, where=[s >= 1 for s in speedups], \n",
    "                  alpha=0.3, color='#2ECC71', label='Rust faster')\n",
    "ax2.fill_between(sim_counts, speedups, 1, where=[s < 1 for s in speedups], \n",
    "                  alpha=0.3, color='#E74C3C', label='Python faster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Profiling: Where is Time Spent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile a single search to understand time breakdown\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "config = MCTSConfig(num_simulations=100)\n",
    "python_mcts = BatchedMCTS(game, config, use_transposition_table=False)\n",
    "\n",
    "state = game.initial_state()\n",
    "states = np.stack([state] * 64)\n",
    "\n",
    "# Profile Python MCTS\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(5):\n",
    "        _ = python_mcts.search(states, model, add_noise=False)\n",
    "\n",
    "pr.disable()\n",
    "\n",
    "s = StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')\n",
    "ps.print_stats(20)\n",
    "print(\"Python MCTS Profile (top 20 by cumulative time):\")\n",
    "print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Rust MCTS Backend Performance:\")\n",
    "print()\n",
    "\n",
    "# Find best speedup\n",
    "best_batch = max(batch_results, key=lambda x: x['speedup'])\n",
    "print(f\"  Best speedup by batch size: {best_batch['speedup']:.2f}x at batch_size={best_batch['batch_size']}\")\n",
    "\n",
    "best_sim = max(sim_results, key=lambda x: x['speedup'])\n",
    "print(f\"  Best speedup by sim count:  {best_sim['speedup']:.2f}x at {best_sim['num_sims']} simulations\")\n",
    "\n",
    "print()\n",
    "print(\"Key Observations:\")\n",
    "print(\"  - Rust MCTS runs entire search loop in Rust\")\n",
    "print(\"  - Only calls back to Python for NN inference\")\n",
    "print(\"  - Speedup improves with larger batch sizes\")\n",
    "print()\n",
    "print(\"Future Optimizations:\")\n",
    "print(\"  - Rayon parallelization for tree selection\")\n",
    "print(\"  - Virtual loss for multi-simulation batching\")\n",
    "print(\"  - Bayesian MCTS (TTTS) variant\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
