{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rust MCTS Virtual Loss Batching Test\n",
    "\n",
    "Tests that PUCT and TTTS Rust backends maintain playing quality with large batch sizes.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Does increasing `leaves_per_batch` hurt game-playing performance?\n",
    "2. How does training speed differ between PUCT and TTTS at various batch sizes?\n",
    "3. Do models trained with high batch multipliers learn as well?\n",
    "\n",
    "**Setup:** Use `Runtime > Change runtime type > GPU` for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Rust toolchain\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Verify Rust installation\n",
    "!rustc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository\n!git clone https://github.com/caldred/nanozero.git\n%cd nanozero\n\n# Install Python dependencies\n# Pin sympy<1.13 to avoid PyTorch compatibility issues\n!pip install -q numpy scipy maturin \"sympy<1.13\"\n\n# Build and install Rust extension\n%cd nanozero-mcts-rs\n!maturin build --release\n!pip install target/wheels/nanozero_mcts_rs-*.whl\n%cd ..\n\n# Verify Rust backend is available\n!python -c \"from nanozero_mcts_rs import RustBatchedMCTS, RustBayesianMCTS; print('Rust backends loaded!')\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport time\nfrom scipy import stats\nfrom nanozero.game import get_game\nfrom nanozero.model import AlphaZeroTransformer\nfrom nanozero.mcts import BatchedMCTS, BayesianMCTS\nfrom nanozero.common import sample_action\nfrom nanozero.config import get_model_config, MCTSConfig, BayesianMCTSConfig\nfrom nanozero.replay import ReplayBuffer\n\n# Aliases for backwards compatibility with notebook\nRustBatchedMCTS = BatchedMCTS\nRustBayesianMCTS = BayesianMCTS\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.inference_mode()\ndef self_play_games(game, model, mcts, num_games, temperature_threshold=15, \n                    parallel_games=32, is_bayesian=False):\n    \"\"\"\n    Play multiple games of self-play in parallel.\n    Works with both PUCT and Bayesian MCTS.\n    \"\"\"\n    model.eval()\n    all_examples = []\n    games_completed = 0\n\n    n_parallel = min(parallel_games, num_games)\n    states = [game.initial_state() for _ in range(n_parallel)]\n    move_counts = [0] * n_parallel\n    game_examples = [[] for _ in range(n_parallel)]\n\n    while games_completed < num_games:\n        active_indices = [i for i, s in enumerate(states) if not game.is_terminal(s)]\n        if not active_indices:\n            break\n\n        # Batch all active states for MCTS\n        active_states = np.stack([states[i] for i in active_indices])\n        \n        if is_bayesian:\n            # Bayesian MCTS doesn't use add_noise parameter\n            policies = mcts.search(active_states, model)\n        else:\n            # PUCT: add noise only at move 0\n            noise_mask = [move_counts[i] == 0 for i in active_indices]\n            if all(noise_mask):\n                policies = mcts.search(active_states, model, add_noise=True)\n            elif not any(noise_mask):\n                policies = mcts.search(active_states, model, add_noise=False)\n            else:\n                # Mixed: do two searches\n                policies = np.zeros((len(active_indices), game.config.action_size), dtype=np.float32)\n                noise_idx = [i for i, m in enumerate(noise_mask) if m]\n                no_noise_idx = [i for i, m in enumerate(noise_mask) if not m]\n                \n                if noise_idx:\n                    noise_states = np.stack([active_states[i] for i in noise_idx])\n                    noise_policies = mcts.search(noise_states, model, add_noise=True)\n                    for j, idx in enumerate(noise_idx):\n                        policies[idx] = noise_policies[j]\n                \n                if no_noise_idx:\n                    no_noise_states = np.stack([active_states[i] for i in no_noise_idx])\n                    no_noise_policies = mcts.search(no_noise_states, model, add_noise=False)\n                    for j, idx in enumerate(no_noise_idx):\n                        policies[idx] = no_noise_policies[j]\n\n        # Process each active game\n        for idx, game_idx in enumerate(active_indices):\n            state = states[game_idx]\n            policy = policies[idx]\n            player = game.current_player(state)\n            move_count = move_counts[game_idx]\n\n            # Store example\n            canonical = game.canonical_state(state)\n            game_examples[game_idx].append((canonical.copy(), policy.copy(), player))\n\n            # Sample action\n            temperature = 1.0 if move_count < temperature_threshold else 0.0\n            action = sample_action(policy, temperature=temperature)\n\n            states[game_idx] = game.next_state(state, action)\n            move_counts[game_idx] += 1\n\n        # Check for finished games\n        for i in range(n_parallel):\n            if game.is_terminal(states[i]) and game_examples[i]:\n                reward = game.terminal_reward(states[i])\n                final_player = game.current_player(states[i])\n\n                for canonical, policy, player in game_examples[i]:\n                    value = reward if player == final_player else -reward\n                    for sym_state, sym_policy in game.symmetries(canonical, policy):\n                        all_examples.append((sym_state, sym_policy, value))\n\n                games_completed += 1\n\n                if games_completed < num_games:\n                    states[i] = game.initial_state()\n                    move_counts[i] = 0\n                    game_examples[i] = []\n\n    return all_examples\n\n\ndef train_step(model, optimizer, states, policies, values, action_masks, device):\n    \"\"\"Single training step.\"\"\"\n    model.train()\n    \n    states = states.to(device)\n    policies = policies.to(device)\n    values = values.to(device)\n    action_masks = action_masks.to(device)\n    \n    optimizer.zero_grad()\n    \n    pred_log_policies, pred_values = model(states, action_masks)\n    policy_loss = -torch.mean(torch.sum(policies * pred_log_policies, dim=1))\n    value_loss = F.mse_loss(pred_values.squeeze(-1), values)\n    loss = policy_loss + value_loss\n    \n    loss.backward()\n    optimizer.step()\n    \n    return loss.item(), policy_loss.item(), value_loss.item()\n\n\n@torch.inference_mode()\ndef evaluate_vs_random(game, model, mcts, num_games=50, is_bayesian=False):\n    \"\"\"\n    Evaluate model against random player.\n    Runs all games in parallel with batched MCTS.\n    \"\"\"\n    model.eval()\n    \n    # Initialize all games\n    states = [game.initial_state() for _ in range(num_games)]\n    model_players = [1 if i % 2 == 0 else -1 for i in range(num_games)]\n    results = [None] * num_games  # None = ongoing, 1 = win, 0 = draw, -1 = loss\n    \n    while any(r is None for r in results):\n        # Find games where it's the model's turn\n        model_turn_indices = []\n        random_turn_indices = []\n        \n        for i, (state, result) in enumerate(zip(states, results)):\n            if result is not None:\n                continue\n            if game.is_terminal(state):\n                # Game just ended\n                reward = game.terminal_reward(state)\n                final_player = game.current_player(state)\n                model_result = reward if final_player == model_players[i] else -reward\n                results[i] = model_result\n                continue\n            \n            current = game.current_player(state)\n            if current == model_players[i]:\n                model_turn_indices.append(i)\n            else:\n                random_turn_indices.append(i)\n        \n        # Batch MCTS for all model moves\n        if model_turn_indices:\n            model_states = np.stack([states[i] for i in model_turn_indices])\n            if is_bayesian:\n                policies = mcts.search(model_states, model)\n            else:\n                policies = mcts.search(model_states, model, add_noise=False)\n            \n            for idx, game_idx in enumerate(model_turn_indices):\n                action = sample_action(policies[idx], temperature=0)\n                states[game_idx] = game.next_state(states[game_idx], action)\n        \n        # Random moves (no batching needed)\n        for game_idx in random_turn_indices:\n            legal = game.legal_actions(states[game_idx])\n            action = np.random.choice(legal)\n            states[game_idx] = game.next_state(states[game_idx], action)\n    \n    wins = sum(1 for r in results if r > 0)\n    return wins / num_games\n\n\nprint(\"Training infrastructure ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import copy\n\ndef train_model(game, model, mcts, num_iterations=10, games_per_iter=50, \n                training_steps=50, batch_size=32, parallel_games=32,\n                eval_interval=5, is_bayesian=False, verbose=True, save_checkpoints=True):\n    \"\"\"\n    Train a model using self-play.\n    \n    Returns:\n        dict with training metrics and checkpoints\n    \"\"\"\n    buffer = ReplayBuffer(100000)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n    \n    metrics = {\n        'self_play_times': [],\n        'train_times': [],\n        'losses': [],\n        'win_rates': [],\n        'checkpoints': {},  # iteration -> model state dict\n    }\n    \n    for iteration in range(num_iterations):\n        if verbose:\n            print(f\"  Iteration {iteration + 1}/{num_iterations}\", end=\" \")\n        \n        # Self-play\n        sp_start = time.perf_counter()\n        examples = self_play_games(\n            game, model, mcts,\n            num_games=games_per_iter,\n            parallel_games=parallel_games,\n            is_bayesian=is_bayesian\n        )\n        sp_time = time.perf_counter() - sp_start\n        metrics['self_play_times'].append(sp_time)\n        \n        # Add to buffer\n        for state, policy, value in examples:\n            buffer.push(state, policy, value)\n        \n        # Training\n        if len(buffer) >= batch_size:\n            train_start = time.perf_counter()\n            total_loss = 0\n            \n            for _ in range(training_steps):\n                states, policies, values = buffer.sample(batch_size)\n                \n                state_tensors = torch.stack([game.to_tensor(s) for s in states])\n                policy_tensors = torch.from_numpy(policies).float()\n                value_tensors = torch.from_numpy(values).float()\n                action_masks = torch.stack([\n                    torch.from_numpy(game.legal_actions_mask(s)) for s in states\n                ]).float()\n                \n                loss, _, _ = train_step(\n                    model, optimizer,\n                    state_tensors, policy_tensors, value_tensors,\n                    action_masks, device\n                )\n                total_loss += loss\n            \n            train_time = time.perf_counter() - train_start\n            avg_loss = total_loss / training_steps\n            metrics['train_times'].append(train_time)\n            metrics['losses'].append(avg_loss)\n            \n            # Clear MCTS cache\n            if hasattr(mcts, 'clear_cache'):\n                mcts.clear_cache()\n        \n        # Evaluate at specified interval\n        if (iteration + 1) % eval_interval == 0 or iteration == num_iterations - 1:\n            win_rate = evaluate_vs_random(game, model, mcts, num_games=50, is_bayesian=is_bayesian)\n            metrics['win_rates'].append((iteration + 1, win_rate))\n            \n            # Save checkpoint\n            if save_checkpoints:\n                metrics['checkpoints'][iteration + 1] = copy.deepcopy(model.state_dict())\n            \n            if verbose:\n                print(f\"| WR: {win_rate:.0%}\", end=\"\")\n        \n        if verbose:\n            print(f\" | SP: {sp_time:.1f}s\")\n    \n    return metrics\n\n\ndef load_checkpoint(model, state_dict):\n    \"\"\"Load a checkpoint into a model (returns a new model copy).\"\"\"\n    new_model = copy.deepcopy(model)\n    new_model.load_state_dict(state_dict)\n    new_model.eval()\n    return new_model\n\n\nprint(\"Training function ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use Connect4 for more meaningful comparison\ngame = get_game('connect4')\nprint(f\"Game: Connect4\")\nprint(f\"Board size: {game.config.board_height}x{game.config.board_width}\")\nprint(f\"Action size: {game.config.action_size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Comparison: PUCT vs TTTS at Different Batch Sizes\n",
    "\n",
    "Train models with different `leaves_per_batch` settings and compare:\n",
    "1. Training speed (games/second)\n",
    "2. Final model quality (win rate vs random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nNUM_ITERATIONS = 100\nGAMES_PER_ITER = 100\nTRAINING_STEPS = 100\nMCTS_SIMS = 500\nPARALLEL_GAMES = 64\nBATCH_SIZE = 128\nEVAL_INTERVAL = 10\n\nprint(f\"Training config:\")\nprint(f\"  Iterations: {NUM_ITERATIONS}\")\nprint(f\"  Games/iter: {GAMES_PER_ITER}\")\nprint(f\"  Training steps: {TRAINING_STEPS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  MCTS sims: {MCTS_SIMS}\")\nprint(f\"  Parallel games: {PARALLEL_GAMES}\")\nprint(f\"  Eval interval: {EVAL_INTERVAL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_fresh_model(game):\n    \"\"\"Create a fresh model with random weights.\"\"\"\n    model_config = get_model_config(game.config, n_layer=4)\n    model = AlphaZeroTransformer(model_config).to(device)\n    return model\n\n# Store results\nall_results = {}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train PUCT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PUCT Training at Different Batch Sizes\")\nprint(\"=\" * 60)\n\npuct_models = {}\npuct_metrics = {}\n\nfor mult in [1, 4, 8]:\n    print(f\"\\nTraining PUCT with {mult}x leaves_per_batch...\")\n    \n    # Fresh model\n    model = create_fresh_model(game)\n    \n    # Create MCTS with specified batch size\n    # Use game-appropriate Dirichlet alpha: ~10/action_size\n    leaves_per_batch = PARALLEL_GAMES * mult\n    config = MCTSConfig(\n        num_simulations=MCTS_SIMS,\n        dirichlet_alpha=10.0 / game.config.action_size,  # ~1.4 for Connect4\n        c_puct=1.5,\n    )\n    mcts = RustBatchedMCTS(game, config, leaves_per_batch=leaves_per_batch)\n    \n    # Train\n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    metrics = train_model(\n        game, model, mcts,\n        num_iterations=NUM_ITERATIONS,\n        games_per_iter=GAMES_PER_ITER,\n        training_steps=TRAINING_STEPS,\n        batch_size=BATCH_SIZE,\n        parallel_games=PARALLEL_GAMES,\n        eval_interval=EVAL_INTERVAL,\n        is_bayesian=False\n    )\n    \n    puct_models[mult] = model\n    puct_metrics[mult] = metrics\n    \n    avg_sp_time = np.mean(metrics['self_play_times'])\n    final_wr = metrics['win_rates'][-1][1] if metrics['win_rates'] else 0\n    print(f\"  Avg self-play time: {avg_sp_time:.2f}s | Final win rate: {final_wr:.0%}\")\n\nall_results['puct'] = {'models': puct_models, 'metrics': puct_metrics}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train TTTS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"TTTS Training at Different Batch Sizes\")\nprint(\"=\" * 60)\n\nttts_models = {}\nttts_metrics = {}\n\nfor mult in [1, 4, 8]:\n    print(f\"\\nTraining TTTS with {mult}x leaves_per_batch...\")\n    \n    # Fresh model\n    model = create_fresh_model(game)\n    \n    # Create MCTS with specified batch size\n    leaves_per_batch = PARALLEL_GAMES * mult\n    config = BayesianMCTSConfig(num_simulations=MCTS_SIMS)\n    mcts = RustBayesianMCTS(game, config, leaves_per_batch=leaves_per_batch)\n    \n    # Train\n    np.random.seed(42)\n    torch.manual_seed(42)\n    \n    metrics = train_model(\n        game, model, mcts,\n        num_iterations=NUM_ITERATIONS,\n        games_per_iter=GAMES_PER_ITER,\n        training_steps=TRAINING_STEPS,\n        batch_size=BATCH_SIZE,\n        parallel_games=PARALLEL_GAMES,\n        eval_interval=EVAL_INTERVAL,\n        is_bayesian=True\n    )\n    \n    ttts_models[mult] = model\n    ttts_metrics[mult] = metrics\n    \n    avg_sp_time = np.mean(metrics['self_play_times'])\n    final_wr = metrics['win_rates'][-1][1] if metrics['win_rates'] else 0\n    print(f\"  Avg self-play time: {avg_sp_time:.2f}s | Final win rate: {final_wr:.0%}\")\n\nall_results['ttts'] = {'models': ttts_models, 'metrics': ttts_metrics}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Trained Models in Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.inference_mode()\ndef run_arena(game, model1, mcts1, model2, mcts2, num_games, \n              is_bayesian1=False, is_bayesian2=False,\n              player1_name=\"P1\", player2_name=\"P2\", verbose=True):\n    \"\"\"\n    Run arena matches between two models.\n    All games run in parallel with batched MCTS.\n    \"\"\"\n    # Initialize all games\n    states = [game.initial_state() for _ in range(num_games)]\n    p1_colors = [1 if i % 2 == 0 else -1 for i in range(num_games)]\n    results = [None] * num_games  # None = ongoing\n    \n    while any(r is None for r in results):\n        # Separate games by whose turn it is\n        p1_turn_indices = []\n        p2_turn_indices = []\n        \n        for i, (state, result) in enumerate(zip(states, results)):\n            if result is not None:\n                continue\n            if game.is_terminal(state):\n                # Game just ended - record result from P1's perspective\n                reward = game.terminal_reward(state)\n                final_player = game.current_player(state)\n                p1_result = reward if final_player == p1_colors[i] else -reward\n                results[i] = p1_result\n                continue\n            \n            current = game.current_player(state)\n            if current == p1_colors[i]:\n                p1_turn_indices.append(i)\n            else:\n                p2_turn_indices.append(i)\n        \n        # Batch MCTS for player 1's moves\n        if p1_turn_indices:\n            p1_states = np.stack([states[i] for i in p1_turn_indices])\n            if is_bayesian1:\n                policies = mcts1.search(p1_states, model1)\n            else:\n                policies = mcts1.search(p1_states, model1, add_noise=False)\n            \n            for idx, game_idx in enumerate(p1_turn_indices):\n                action = sample_action(policies[idx], temperature=0)\n                states[game_idx] = game.next_state(states[game_idx], action)\n        \n        # Batch MCTS for player 2's moves\n        if p2_turn_indices:\n            p2_states = np.stack([states[i] for i in p2_turn_indices])\n            if is_bayesian2:\n                policies = mcts2.search(p2_states, model2)\n            else:\n                policies = mcts2.search(p2_states, model2, add_noise=False)\n            \n            for idx, game_idx in enumerate(p2_turn_indices):\n                action = sample_action(policies[idx], temperature=0)\n                states[game_idx] = game.next_state(states[game_idx], action)\n    \n    # Count results\n    wins = sum(1 for r in results if r > 0)\n    draws = sum(1 for r in results if r == 0)\n    losses = sum(1 for r in results if r < 0)\n    \n    if verbose:\n        print(f\"  {num_games} games: {player1_name} {wins}W/{draws}D/{losses}L\")\n    \n    return wins, draws, losses"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 PUCT: Compare 1x vs 8x trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PUCT Arena: 1x-trained vs 8x-trained models\")\nprint(\"=\" * 60)\nprint(\"\\nBoth use 1x leaves_per_batch for fair evaluation.\\n\")\n\n# Use same MCTS config for fair comparison (with tuned hyperparams)\neval_config = MCTSConfig(\n    num_simulations=MCTS_SIMS,\n    dirichlet_alpha=10.0 / game.config.action_size,\n    c_puct=1.5,\n)\neval_mcts = RustBatchedMCTS(game, eval_config, leaves_per_batch=PARALLEL_GAMES)\n\nnp.random.seed(42)\nwins, draws, losses = run_arena(\n    game,\n    puct_models[8], eval_mcts,\n    puct_models[1], eval_mcts,\n    num_games=100,\n    is_bayesian1=False, is_bayesian2=False,\n    player1_name=\"8x-trained\", player2_name=\"1x-trained\"\n)\n\ndecisive = wins + losses\nwr_8x = wins / decisive if decisive > 0 else 0.5\n\nprint(f\"\\nResults (8x-trained perspective): {wins}W / {draws}D / {losses}L\")\nprint(f\"8x-trained decisive win rate: {wr_8x:.1%}\")\n\nif decisive > 0:\n    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n    print(f\"p-value: {p_value:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TTTS: Compare 1x vs 8x trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"TTTS Arena: 1x-trained vs 8x-trained models\")\nprint(\"=\" * 60)\nprint(\"\\nBoth use 1x leaves_per_batch for fair evaluation.\\n\")\n\n# Use same MCTS config for fair comparison\neval_config = BayesianMCTSConfig(num_simulations=MCTS_SIMS)\neval_mcts = RustBayesianMCTS(game, eval_config, leaves_per_batch=PARALLEL_GAMES)\n\nnp.random.seed(42)\nwins, draws, losses = run_arena(\n    game,\n    ttts_models[8], eval_mcts,\n    ttts_models[1], eval_mcts,\n    num_games=100,\n    is_bayesian1=True, is_bayesian2=True,\n    player1_name=\"8x-trained\", player2_name=\"1x-trained\"\n)\n\ndecisive = wins + losses\nwr_8x = wins / decisive if decisive > 0 else 0.5\n\nprint(f\"\\nResults (8x-trained perspective): {wins}W / {draws}D / {losses}L\")\nprint(f\"8x-trained decisive win rate: {wr_8x:.1%}\")\n\nif decisive > 0:\n    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n    print(f\"p-value: {p_value:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cross-algorithm: Best PUCT vs Best TTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PUCT vs TTTS: Best models from each algorithm\")\nprint(\"=\" * 60)\n\n# Find best performing model from each\nbest_puct_mult = max(puct_metrics.keys(), \n                     key=lambda m: puct_metrics[m]['win_rates'][-1][1] if puct_metrics[m]['win_rates'] else 0)\nbest_ttts_mult = max(ttts_metrics.keys(),\n                     key=lambda m: ttts_metrics[m]['win_rates'][-1][1] if ttts_metrics[m]['win_rates'] else 0)\n\nprint(f\"\\nBest PUCT: {best_puct_mult}x (WR: {puct_metrics[best_puct_mult]['win_rates'][-1][1]:.0%})\")\nprint(f\"Best TTTS: {best_ttts_mult}x (WR: {ttts_metrics[best_ttts_mult]['win_rates'][-1][1]:.0%})\")\nprint(\"\\nPlaying 100 games...\\n\")\n\npuct_eval = RustBatchedMCTS(game, MCTSConfig(\n    num_simulations=MCTS_SIMS,\n    dirichlet_alpha=10.0 / game.config.action_size,\n    c_puct=1.5,\n), leaves_per_batch=PARALLEL_GAMES)\nttts_eval = RustBayesianMCTS(game, BayesianMCTSConfig(num_simulations=MCTS_SIMS), leaves_per_batch=PARALLEL_GAMES)\n\nnp.random.seed(42)\nwins, draws, losses = run_arena(\n    game,\n    ttts_models[best_ttts_mult], ttts_eval,\n    puct_models[best_puct_mult], puct_eval,\n    num_games=100,\n    is_bayesian1=True, is_bayesian2=False,\n    player1_name=\"TTTS\", player2_name=\"PUCT\"\n)\n\ndecisive = wins + losses\nttts_wr = wins / decisive if decisive > 0 else 0.5\n\nprint(f\"\\nResults (TTTS perspective): {wins}W / {draws}D / {losses}L\")\nprint(f\"TTTS decisive win rate: {ttts_wr:.1%}\")\n\nif decisive > 0:\n    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n    print(f\"p-value: {p_value:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Checkpoint Progression: Early vs Late Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 60)\nprint(\"Checkpoint Progression Arena\")\nprint(\"=\" * 60)\nprint(\"\\nCompare early checkpoints vs final models to verify learning.\\n\")\n\n# Use the best batch multiplier for each algorithm\nbest_puct_mult = max(puct_metrics.keys(), \n                     key=lambda m: puct_metrics[m]['win_rates'][-1][1] if puct_metrics[m]['win_rates'] else 0)\nbest_ttts_mult = max(ttts_metrics.keys(),\n                     key=lambda m: ttts_metrics[m]['win_rates'][-1][1] if ttts_metrics[m]['win_rates'] else 0)\n\n# Get checkpoints\npuct_checkpoints = puct_metrics[best_puct_mult]['checkpoints']\nttts_checkpoints = ttts_metrics[best_ttts_mult]['checkpoints']\n\ncheckpoint_iters = sorted(puct_checkpoints.keys())\nprint(f\"Available checkpoints: {checkpoint_iters}\")\n\nif len(checkpoint_iters) >= 2:\n    early_iter = checkpoint_iters[0]\n    final_iter = checkpoint_iters[-1]\n    \n    # Create models from checkpoints\n    template_model = create_fresh_model(game)\n    \n    # PUCT: Early vs Final\n    print(f\"\\n--- PUCT: Iteration {early_iter} vs Iteration {final_iter} ---\")\n    puct_early = load_checkpoint(template_model, puct_checkpoints[early_iter])\n    puct_final = load_checkpoint(template_model, puct_checkpoints[final_iter])\n    \n    puct_eval = RustBatchedMCTS(game, MCTSConfig(\n        num_simulations=MCTS_SIMS,\n        dirichlet_alpha=10.0 / game.config.action_size,\n        c_puct=1.5,\n    ), leaves_per_batch=PARALLEL_GAMES)\n    \n    np.random.seed(42)\n    wins, draws, losses = run_arena(\n        game,\n        puct_final, puct_eval,\n        puct_early, puct_eval,\n        num_games=100,\n        is_bayesian1=False, is_bayesian2=False,\n        player1_name=f\"iter-{final_iter}\", player2_name=f\"iter-{early_iter}\"\n    )\n    \n    decisive = wins + losses\n    final_wr = wins / decisive if decisive > 0 else 0.5\n    print(f\"  Final model decisive win rate: {final_wr:.1%}\")\n    \n    # TTTS: Early vs Final\n    print(f\"\\n--- TTTS: Iteration {early_iter} vs Iteration {final_iter} ---\")\n    ttts_early = load_checkpoint(template_model, ttts_checkpoints[early_iter])\n    ttts_final = load_checkpoint(template_model, ttts_checkpoints[final_iter])\n    \n    ttts_eval = RustBayesianMCTS(game, BayesianMCTSConfig(num_simulations=MCTS_SIMS), leaves_per_batch=PARALLEL_GAMES)\n    \n    np.random.seed(42)\n    wins, draws, losses = run_arena(\n        game,\n        ttts_final, ttts_eval,\n        ttts_early, ttts_eval,\n        num_games=100,\n        is_bayesian1=True, is_bayesian2=True,\n        player1_name=f\"iter-{final_iter}\", player2_name=f\"iter-{early_iter}\"\n    )\n    \n    decisive = wins + losses\n    final_wr = wins / decisive if decisive > 0 else 0.5\n    print(f\"  Final model decisive win rate: {final_wr:.1%}\")\n    \n    # Full checkpoint ladder for best algorithm\n    print(f\"\\n--- TTTS Checkpoint Ladder (best performer) ---\")\n    print(\"Each checkpoint plays against all earlier checkpoints:\\n\")\n    \n    ladder_results = {}\n    for i, later_iter in enumerate(checkpoint_iters[1:], 1):\n        for earlier_iter in checkpoint_iters[:i]:\n            later_model = load_checkpoint(template_model, ttts_checkpoints[later_iter])\n            earlier_model = load_checkpoint(template_model, ttts_checkpoints[earlier_iter])\n            \n            wins, draws, losses = run_arena(\n                game,\n                later_model, ttts_eval,\n                earlier_model, ttts_eval,\n                num_games=50,\n                is_bayesian1=True, is_bayesian2=True,\n                player1_name=f\"iter-{later_iter}\", player2_name=f\"iter-{earlier_iter}\",\n                verbose=False\n            )\n            \n            decisive = wins + losses\n            wr = wins / decisive if decisive > 0 else 0.5\n            ladder_results[(later_iter, earlier_iter)] = (wins, draws, losses, wr)\n            print(f\"  iter-{later_iter} vs iter-{earlier_iter}: {wins}W/{draws}D/{losses}L (WR: {wr:.0%})\")\n\nelse:\n    print(\"Not enough checkpoints to compare.\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: Training with Different Batch Sizes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n Training Speed (avg self-play time per iteration):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Multiplier':<12} {'PUCT (s)':<15} {'TTTS (s)':<15}\")\n",
    "for mult in [1, 4, 8]:\n",
    "    puct_time = np.mean(puct_metrics[mult]['self_play_times'])\n",
    "    ttts_time = np.mean(ttts_metrics[mult]['self_play_times'])\n",
    "    print(f\"{mult}x{'':<10} {puct_time:<15.2f} {ttts_time:<15.2f}\")\n",
    "\n",
    "print(\"\\n Final Win Rate vs Random:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Multiplier':<12} {'PUCT':<15} {'TTTS':<15}\")\n",
    "for mult in [1, 4, 8]:\n",
    "    puct_wr = puct_metrics[mult]['win_rates'][-1][1] if puct_metrics[mult]['win_rates'] else 0\n",
    "    ttts_wr = ttts_metrics[mult]['win_rates'][-1][1] if ttts_metrics[mult]['win_rates'] else 0\n",
    "    print(f\"{mult}x{'':<10} {puct_wr:<15.0%} {ttts_wr:<15.0%}\")\n",
    "\n",
    "print(\"\\n Conclusions:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"- Higher batch multipliers provide faster training\")\n",
    "print(\"- Model quality should be similar across batch sizes\")\n",
    "print(\"- PUCT and TTTS show different sensitivity to batching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training speed comparison\n",
    "ax1 = axes[0, 0]\n",
    "mults = [1, 4, 8]\n",
    "puct_times = [np.mean(puct_metrics[m]['self_play_times']) for m in mults]\n",
    "ttts_times = [np.mean(ttts_metrics[m]['self_play_times']) for m in mults]\n",
    "\n",
    "x = np.arange(len(mults))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, puct_times, width, label='PUCT', color='#3498DB')\n",
    "ax1.bar(x + width/2, ttts_times, width, label='TTTS', color='#E67E22')\n",
    "ax1.set_xlabel('leaves_per_batch Multiplier')\n",
    "ax1.set_ylabel('Self-play Time (s)')\n",
    "ax1.set_title('Training Speed by Batch Size')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'{m}x' for m in mults])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Final win rates\n",
    "ax2 = axes[0, 1]\n",
    "puct_wrs = [puct_metrics[m]['win_rates'][-1][1] if puct_metrics[m]['win_rates'] else 0 for m in mults]\n",
    "ttts_wrs = [ttts_metrics[m]['win_rates'][-1][1] if ttts_metrics[m]['win_rates'] else 0 for m in mults]\n",
    "\n",
    "ax2.bar(x - width/2, puct_wrs, width, label='PUCT', color='#3498DB')\n",
    "ax2.bar(x + width/2, ttts_wrs, width, label='TTTS', color='#E67E22')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('leaves_per_batch Multiplier')\n",
    "ax2.set_ylabel('Win Rate vs Random')\n",
    "ax2.set_title('Final Model Quality')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'{m}x' for m in mults])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Learning curves - PUCT\n",
    "ax3 = axes[1, 0]\n",
    "for mult in mults:\n",
    "    iters = [wr[0] for wr in puct_metrics[mult]['win_rates']]\n",
    "    wrs = [wr[1] for wr in puct_metrics[mult]['win_rates']]\n",
    "    ax3.plot(iters, wrs, 'o-', label=f'{mult}x', linewidth=2)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Win Rate vs Random')\n",
    "ax3.set_title('PUCT Learning Curves')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Learning curves - TTTS\n",
    "ax4 = axes[1, 1]\n",
    "for mult in mults:\n",
    "    iters = [wr[0] for wr in ttts_metrics[mult]['win_rates']]\n",
    "    wrs = [wr[1] for wr in ttts_metrics[mult]['win_rates']]\n",
    "    ax4.plot(iters, wrs, 'o-', label=f'{mult}x', linewidth=2)\n",
    "ax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('Win Rate vs Random')\n",
    "ax4.set_title('TTTS Learning Curves')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}