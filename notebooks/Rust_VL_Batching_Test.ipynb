{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rust MCTS Virtual Loss Batching Test\n",
    "\n",
    "Tests that PUCT and TTTS Rust backends maintain playing quality with large batch sizes.\n",
    "\n",
    "**Key Questions:**\n",
    "1. Does increasing `leaves_per_batch` hurt game-playing performance?\n",
    "2. How does training speed differ between PUCT and TTTS at various batch sizes?\n",
    "3. Do models trained with high batch multipliers learn as well?\n",
    "\n",
    "**Setup:** Use `Runtime > Change runtime type > GPU` for best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Rust toolchain\n",
    "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
    "import os\n",
    "os.environ[\"PATH\"] = f\"{os.environ['HOME']}/.cargo/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Verify Rust installation\n",
    "!rustc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/caldred/nanozero.git\n",
    "%cd nanozero\n",
    "\n",
    "# Install Python dependencies\n",
    "!pip install -q numpy scipy maturin\n",
    "\n",
    "# Build and install Rust extension\n",
    "%cd nanozero-mcts-rs\n",
    "!maturin build --release\n",
    "!pip install target/wheels/nanozero_mcts_rs-*.whl\n",
    "%cd ..\n",
    "\n",
    "# Verify Rust backend is available\n",
    "!python -c \"from nanozero_mcts_rs import RustBatchedMCTS, RustBayesianMCTS; print('Rust backends loaded!')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport time\nfrom scipy import stats\nfrom nanozero.game import get_game\nfrom nanozero.model import AlphaZeroTransformer\nfrom nanozero.mcts import BatchedMCTS, BayesianMCTS\nfrom nanozero.common import sample_action\nfrom nanozero.config import get_model_config, MCTSConfig, BayesianMCTSConfig\nfrom nanozero.replay import ReplayBuffer\n\n# Aliases for backwards compatibility with notebook\nRustBatchedMCTS = BatchedMCTS\nRustBayesianMCTS = BayesianMCTS\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def self_play_games(game, model, mcts, num_games, temperature_threshold=15, \n",
    "                    parallel_games=32, is_bayesian=False):\n",
    "    \"\"\"\n",
    "    Play multiple games of self-play in parallel.\n",
    "    Works with both PUCT and Bayesian MCTS.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_examples = []\n",
    "    games_completed = 0\n",
    "\n",
    "    n_parallel = min(parallel_games, num_games)\n",
    "    states = [game.initial_state() for _ in range(n_parallel)]\n",
    "    move_counts = [0] * n_parallel\n",
    "    game_examples = [[] for _ in range(n_parallel)]\n",
    "\n",
    "    while games_completed < num_games:\n",
    "        active_indices = [i for i, s in enumerate(states) if not game.is_terminal(s)]\n",
    "        if not active_indices:\n",
    "            break\n",
    "\n",
    "        # Batch all active states for MCTS\n",
    "        active_states = np.stack([states[i] for i in active_indices])\n",
    "        \n",
    "        if is_bayesian:\n",
    "            # Bayesian MCTS doesn't use add_noise parameter\n",
    "            policies = mcts.search(active_states, model)\n",
    "        else:\n",
    "            # PUCT: add noise only at move 0\n",
    "            noise_mask = [move_counts[i] == 0 for i in active_indices]\n",
    "            if all(noise_mask):\n",
    "                policies = mcts.search(active_states, model, add_noise=True)\n",
    "            elif not any(noise_mask):\n",
    "                policies = mcts.search(active_states, model, add_noise=False)\n",
    "            else:\n",
    "                # Mixed: do two searches\n",
    "                policies = np.zeros((len(active_indices), game.config.action_size), dtype=np.float32)\n",
    "                noise_idx = [i for i, m in enumerate(noise_mask) if m]\n",
    "                no_noise_idx = [i for i, m in enumerate(noise_mask) if not m]\n",
    "                \n",
    "                if noise_idx:\n",
    "                    noise_states = np.stack([active_states[i] for i in noise_idx])\n",
    "                    noise_policies = mcts.search(noise_states, model, add_noise=True)\n",
    "                    for j, idx in enumerate(noise_idx):\n",
    "                        policies[idx] = noise_policies[j]\n",
    "                \n",
    "                if no_noise_idx:\n",
    "                    no_noise_states = np.stack([active_states[i] for i in no_noise_idx])\n",
    "                    no_noise_policies = mcts.search(no_noise_states, model, add_noise=False)\n",
    "                    for j, idx in enumerate(no_noise_idx):\n",
    "                        policies[idx] = no_noise_policies[j]\n",
    "\n",
    "        # Process each active game\n",
    "        for idx, game_idx in enumerate(active_indices):\n",
    "            state = states[game_idx]\n",
    "            policy = policies[idx]\n",
    "            player = game.current_player(state)\n",
    "            move_count = move_counts[game_idx]\n",
    "\n",
    "            # Store example\n",
    "            canonical = game.canonical_state(state)\n",
    "            game_examples[game_idx].append((canonical.copy(), policy.copy(), player))\n",
    "\n",
    "            # Sample action\n",
    "            temperature = 1.0 if move_count < temperature_threshold else 0.0\n",
    "            action = sample_action(policy, temperature=temperature)\n",
    "\n",
    "            states[game_idx] = game.next_state(state, action)\n",
    "            move_counts[game_idx] += 1\n",
    "\n",
    "        # Check for finished games\n",
    "        for i in range(n_parallel):\n",
    "            if game.is_terminal(states[i]) and game_examples[i]:\n",
    "                reward = game.terminal_reward(states[i])\n",
    "                final_player = game.current_player(states[i])\n",
    "\n",
    "                for canonical, policy, player in game_examples[i]:\n",
    "                    value = reward if player == final_player else -reward\n",
    "                    for sym_state, sym_policy in game.symmetries(canonical, policy):\n",
    "                        all_examples.append((sym_state, sym_policy, value))\n",
    "\n",
    "                games_completed += 1\n",
    "\n",
    "                if games_completed < num_games:\n",
    "                    states[i] = game.initial_state()\n",
    "                    move_counts[i] = 0\n",
    "                    game_examples[i] = []\n",
    "\n",
    "    return all_examples\n",
    "\n",
    "\n",
    "def train_step(model, optimizer, states, policies, values, action_masks, device):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    states = states.to(device)\n",
    "    policies = policies.to(device)\n",
    "    values = values.to(device)\n",
    "    action_masks = action_masks.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred_log_policies, pred_values = model(states, action_masks)\n",
    "    policy_loss = -torch.mean(torch.sum(policies * pred_log_policies, dim=1))\n",
    "    value_loss = F.mse_loss(pred_values.squeeze(-1), values)\n",
    "    loss = policy_loss + value_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), policy_loss.item(), value_loss.item()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_vs_random(game, model, mcts, num_games=50, is_bayesian=False):\n",
    "    \"\"\"Evaluate model against random player.\"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        state = game.initial_state()\n",
    "        model_player = 1 if i % 2 == 0 else -1\n",
    "        \n",
    "        while not game.is_terminal(state):\n",
    "            current = game.current_player(state)\n",
    "            \n",
    "            if current == model_player:\n",
    "                if is_bayesian:\n",
    "                    policy = mcts.search(state[np.newaxis, ...], model)[0]\n",
    "                else:\n",
    "                    policy = mcts.search(state[np.newaxis, ...], model, add_noise=False)[0]\n",
    "                action = sample_action(policy, temperature=0)\n",
    "            else:\n",
    "                legal = game.legal_actions(state)\n",
    "                action = np.random.choice(legal)\n",
    "            \n",
    "            state = game.next_state(state, action)\n",
    "        \n",
    "        reward = game.terminal_reward(state)\n",
    "        final_player = game.current_player(state)\n",
    "        model_result = reward if final_player == model_player else -reward\n",
    "        \n",
    "        if model_result > 0:\n",
    "            wins += 1\n",
    "    \n",
    "    return wins / num_games\n",
    "\n",
    "\n",
    "print(\"Training infrastructure ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(game, model, mcts, num_iterations=10, games_per_iter=50, \n",
    "                training_steps=50, batch_size=32, parallel_games=32,\n",
    "                is_bayesian=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a model using self-play.\n",
    "    \n",
    "    Returns:\n",
    "        dict with training metrics\n",
    "    \"\"\"\n",
    "    buffer = ReplayBuffer(50000)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    metrics = {\n",
    "        'self_play_times': [],\n",
    "        'train_times': [],\n",
    "        'losses': [],\n",
    "        'win_rates': []\n",
    "    }\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        if verbose:\n",
    "            print(f\"  Iteration {iteration + 1}/{num_iterations}\", end=\" \")\n",
    "        \n",
    "        # Self-play\n",
    "        sp_start = time.perf_counter()\n",
    "        examples = self_play_games(\n",
    "            game, model, mcts,\n",
    "            num_games=games_per_iter,\n",
    "            parallel_games=parallel_games,\n",
    "            is_bayesian=is_bayesian\n",
    "        )\n",
    "        sp_time = time.perf_counter() - sp_start\n",
    "        metrics['self_play_times'].append(sp_time)\n",
    "        \n",
    "        # Add to buffer\n",
    "        for state, policy, value in examples:\n",
    "            buffer.push(state, policy, value)\n",
    "        \n",
    "        # Training\n",
    "        if len(buffer) >= batch_size:\n",
    "            train_start = time.perf_counter()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for _ in range(training_steps):\n",
    "                states, policies, values = buffer.sample(batch_size)\n",
    "                \n",
    "                state_tensors = torch.stack([game.to_tensor(s) for s in states])\n",
    "                policy_tensors = torch.from_numpy(policies).float()\n",
    "                value_tensors = torch.from_numpy(values).float()\n",
    "                action_masks = torch.stack([\n",
    "                    torch.from_numpy(game.legal_actions_mask(s)) for s in states\n",
    "                ]).float()\n",
    "                \n",
    "                loss, _, _ = train_step(\n",
    "                    model, optimizer,\n",
    "                    state_tensors, policy_tensors, value_tensors,\n",
    "                    action_masks, device\n",
    "                )\n",
    "                total_loss += loss\n",
    "            \n",
    "            train_time = time.perf_counter() - train_start\n",
    "            avg_loss = total_loss / training_steps\n",
    "            metrics['train_times'].append(train_time)\n",
    "            metrics['losses'].append(avg_loss)\n",
    "            \n",
    "            # Clear MCTS cache\n",
    "            if hasattr(mcts, 'clear_cache'):\n",
    "                mcts.clear_cache()\n",
    "        \n",
    "        # Evaluate every few iterations\n",
    "        if (iteration + 1) % 5 == 0 or iteration == num_iterations - 1:\n",
    "            win_rate = evaluate_vs_random(game, model, mcts, num_games=30, is_bayesian=is_bayesian)\n",
    "            metrics['win_rates'].append((iteration + 1, win_rate))\n",
    "            if verbose:\n",
    "                print(f\"| WR: {win_rate:.0%}\", end=\"\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\" | SP: {sp_time:.1f}s\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TicTacToe for faster iteration (Connect4 takes much longer)\n",
    "game = get_game('tictactoe')\n",
    "print(f\"Game: TicTacToe\")\n",
    "print(f\"Board size: {game.config.board_size}\")\n",
    "print(f\"Action size: {game.config.action_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Comparison: PUCT vs TTTS at Different Batch Sizes\n",
    "\n",
    "Train models with different `leaves_per_batch` settings and compare:\n",
    "1. Training speed (games/second)\n",
    "2. Final model quality (win rate vs random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_ITERATIONS = 15  # Enough to see learning\n",
    "GAMES_PER_ITER = 50\n",
    "TRAINING_STEPS = 50\n",
    "MCTS_SIMS = 50\n",
    "PARALLEL_GAMES = 32\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games/iter: {GAMES_PER_ITER}\")\n",
    "print(f\"  MCTS sims: {MCTS_SIMS}\")\n",
    "print(f\"  Parallel games: {PARALLEL_GAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fresh_model(game):\n",
    "    \"\"\"Create a fresh model with random weights.\"\"\"\n",
    "    model_config = get_model_config(game.config, n_layer=2)\n",
    "    model = AlphaZeroTransformer(model_config).to(device)\n",
    "    return model\n",
    "\n",
    "# Store results\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train PUCT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PUCT Training at Different Batch Sizes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "puct_models = {}\n",
    "puct_metrics = {}\n",
    "\n",
    "for mult in [1, 4, 8]:\n",
    "    print(f\"\\nTraining PUCT with {mult}x leaves_per_batch...\")\n",
    "    \n",
    "    # Fresh model\n",
    "    model = create_fresh_model(game)\n",
    "    \n",
    "    # Create MCTS with specified batch size\n",
    "    leaves_per_batch = PARALLEL_GAMES * mult\n",
    "    config = MCTSConfig(num_simulations=MCTS_SIMS)\n",
    "    mcts = RustBatchedMCTS(game, config, leaves_per_batch=leaves_per_batch)\n",
    "    \n",
    "    # Train\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    metrics = train_model(\n",
    "        game, model, mcts,\n",
    "        num_iterations=NUM_ITERATIONS,\n",
    "        games_per_iter=GAMES_PER_ITER,\n",
    "        training_steps=TRAINING_STEPS,\n",
    "        parallel_games=PARALLEL_GAMES,\n",
    "        is_bayesian=False\n",
    "    )\n",
    "    \n",
    "    puct_models[mult] = model\n",
    "    puct_metrics[mult] = metrics\n",
    "    \n",
    "    avg_sp_time = np.mean(metrics['self_play_times'])\n",
    "    final_wr = metrics['win_rates'][-1][1] if metrics['win_rates'] else 0\n",
    "    print(f\"  Avg self-play time: {avg_sp_time:.2f}s | Final win rate: {final_wr:.0%}\")\n",
    "\n",
    "all_results['puct'] = {'models': puct_models, 'metrics': puct_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train TTTS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TTTS Training at Different Batch Sizes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ttts_models = {}\n",
    "ttts_metrics = {}\n",
    "\n",
    "for mult in [1, 4, 8]:\n",
    "    print(f\"\\nTraining TTTS with {mult}x leaves_per_batch...\")\n",
    "    \n",
    "    # Fresh model\n",
    "    model = create_fresh_model(game)\n",
    "    \n",
    "    # Create MCTS with specified batch size\n",
    "    leaves_per_batch = PARALLEL_GAMES * mult\n",
    "    config = BayesianMCTSConfig(num_simulations=MCTS_SIMS)\n",
    "    mcts = RustBayesianMCTS(game, config, leaves_per_batch=leaves_per_batch)\n",
    "    \n",
    "    # Train\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    metrics = train_model(\n",
    "        game, model, mcts,\n",
    "        num_iterations=NUM_ITERATIONS,\n",
    "        games_per_iter=GAMES_PER_ITER,\n",
    "        training_steps=TRAINING_STEPS,\n",
    "        parallel_games=PARALLEL_GAMES,\n",
    "        is_bayesian=True\n",
    "    )\n",
    "    \n",
    "    ttts_models[mult] = model\n",
    "    ttts_metrics[mult] = metrics\n",
    "    \n",
    "    avg_sp_time = np.mean(metrics['self_play_times'])\n",
    "    final_wr = metrics['win_rates'][-1][1] if metrics['win_rates'] else 0\n",
    "    print(f\"  Avg self-play time: {avg_sp_time:.2f}s | Final win rate: {final_wr:.0%}\")\n",
    "\n",
    "all_results['ttts'] = {'models': ttts_models, 'metrics': ttts_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Trained Models in Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arena(game, model1, mcts1, model2, mcts2, num_games, \n",
    "              is_bayesian1=False, is_bayesian2=False,\n",
    "              player1_name=\"P1\", player2_name=\"P2\", verbose=True):\n",
    "    \"\"\"Run arena matches between two models.\"\"\"\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        state = game.initial_state()\n",
    "        p1_color = 1 if i % 2 == 0 else -1\n",
    "        \n",
    "        while not game.is_terminal(state):\n",
    "            current = game.current_player(state)\n",
    "            \n",
    "            if current == p1_color:\n",
    "                if is_bayesian1:\n",
    "                    policy = mcts1.search(state[np.newaxis, ...], model1)[0]\n",
    "                else:\n",
    "                    policy = mcts1.search(state[np.newaxis, ...], model1, add_noise=False)[0]\n",
    "                action = sample_action(policy, temperature=0)\n",
    "            else:\n",
    "                if is_bayesian2:\n",
    "                    policy = mcts2.search(state[np.newaxis, ...], model2)[0]\n",
    "                else:\n",
    "                    policy = mcts2.search(state[np.newaxis, ...], model2, add_noise=False)[0]\n",
    "                action = sample_action(policy, temperature=0)\n",
    "            \n",
    "            state = game.next_state(state, action)\n",
    "        \n",
    "        reward = game.terminal_reward(state)\n",
    "        final_player = game.current_player(state)\n",
    "        p1_result = reward if final_player == p1_color else -reward\n",
    "        \n",
    "        if p1_result > 0:\n",
    "            wins += 1\n",
    "        elif p1_result < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "        \n",
    "        if verbose and (i + 1) % 20 == 0:\n",
    "            print(f\"  {i+1}/{num_games}: {player1_name} {wins}W/{draws}D/{losses}L\")\n",
    "    \n",
    "    return wins, draws, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 PUCT: Compare 1x vs 8x trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PUCT Arena: 1x-trained vs 8x-trained models\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBoth use 1x leaves_per_batch for fair evaluation.\\n\")\n",
    "\n",
    "# Use same MCTS config for fair comparison\n",
    "eval_config = MCTSConfig(num_simulations=50)\n",
    "eval_mcts = RustBatchedMCTS(game, eval_config, leaves_per_batch=32)\n",
    "\n",
    "np.random.seed(42)\n",
    "wins, draws, losses = run_arena(\n",
    "    game,\n",
    "    puct_models[8], eval_mcts,\n",
    "    puct_models[1], eval_mcts,\n",
    "    num_games=100,\n",
    "    is_bayesian1=False, is_bayesian2=False,\n",
    "    player1_name=\"8x-trained\", player2_name=\"1x-trained\"\n",
    ")\n",
    "\n",
    "decisive = wins + losses\n",
    "wr_8x = wins / decisive if decisive > 0 else 0.5\n",
    "\n",
    "print(f\"\\nResults (8x-trained perspective): {wins}W / {draws}D / {losses}L\")\n",
    "print(f\"8x-trained decisive win rate: {wr_8x:.1%}\")\n",
    "\n",
    "if decisive > 0:\n",
    "    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n",
    "    print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TTTS: Compare 1x vs 8x trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TTTS Arena: 1x-trained vs 8x-trained models\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBoth use 1x leaves_per_batch for fair evaluation.\\n\")\n",
    "\n",
    "# Use same MCTS config for fair comparison\n",
    "eval_config = BayesianMCTSConfig(num_simulations=50)\n",
    "eval_mcts = RustBayesianMCTS(game, eval_config, leaves_per_batch=32)\n",
    "\n",
    "np.random.seed(42)\n",
    "wins, draws, losses = run_arena(\n",
    "    game,\n",
    "    ttts_models[8], eval_mcts,\n",
    "    ttts_models[1], eval_mcts,\n",
    "    num_games=100,\n",
    "    is_bayesian1=True, is_bayesian2=True,\n",
    "    player1_name=\"8x-trained\", player2_name=\"1x-trained\"\n",
    ")\n",
    "\n",
    "decisive = wins + losses\n",
    "wr_8x = wins / decisive if decisive > 0 else 0.5\n",
    "\n",
    "print(f\"\\nResults (8x-trained perspective): {wins}W / {draws}D / {losses}L\")\n",
    "print(f\"8x-trained decisive win rate: {wr_8x:.1%}\")\n",
    "\n",
    "if decisive > 0:\n",
    "    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n",
    "    print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cross-algorithm: Best PUCT vs Best TTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PUCT vs TTTS: Best models from each algorithm\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performing model from each\n",
    "best_puct_mult = max(puct_metrics.keys(), \n",
    "                     key=lambda m: puct_metrics[m]['win_rates'][-1][1] if puct_metrics[m]['win_rates'] else 0)\n",
    "best_ttts_mult = max(ttts_metrics.keys(),\n",
    "                     key=lambda m: ttts_metrics[m]['win_rates'][-1][1] if ttts_metrics[m]['win_rates'] else 0)\n",
    "\n",
    "print(f\"\\nBest PUCT: {best_puct_mult}x (WR: {puct_metrics[best_puct_mult]['win_rates'][-1][1]:.0%})\")\n",
    "print(f\"Best TTTS: {best_ttts_mult}x (WR: {ttts_metrics[best_ttts_mult]['win_rates'][-1][1]:.0%})\")\n",
    "print(\"\\nPlaying 100 games...\\n\")\n",
    "\n",
    "puct_eval = RustBatchedMCTS(game, MCTSConfig(num_simulations=50), leaves_per_batch=32)\n",
    "ttts_eval = RustBayesianMCTS(game, BayesianMCTSConfig(num_simulations=50), leaves_per_batch=32)\n",
    "\n",
    "np.random.seed(42)\n",
    "wins, draws, losses = run_arena(\n",
    "    game,\n",
    "    ttts_models[best_ttts_mult], ttts_eval,\n",
    "    puct_models[best_puct_mult], puct_eval,\n",
    "    num_games=100,\n",
    "    is_bayesian1=True, is_bayesian2=False,\n",
    "    player1_name=\"TTTS\", player2_name=\"PUCT\"\n",
    ")\n",
    "\n",
    "decisive = wins + losses\n",
    "ttts_wr = wins / decisive if decisive > 0 else 0.5\n",
    "\n",
    "print(f\"\\nResults (TTTS perspective): {wins}W / {draws}D / {losses}L\")\n",
    "print(f\"TTTS decisive win rate: {ttts_wr:.1%}\")\n",
    "\n",
    "if decisive > 0:\n",
    "    p_value = stats.binomtest(wins, decisive, 0.5).pvalue\n",
    "    print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: Training with Different Batch Sizes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n Training Speed (avg self-play time per iteration):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Multiplier':<12} {'PUCT (s)':<15} {'TTTS (s)':<15}\")\n",
    "for mult in [1, 4, 8]:\n",
    "    puct_time = np.mean(puct_metrics[mult]['self_play_times'])\n",
    "    ttts_time = np.mean(ttts_metrics[mult]['self_play_times'])\n",
    "    print(f\"{mult}x{'':<10} {puct_time:<15.2f} {ttts_time:<15.2f}\")\n",
    "\n",
    "print(\"\\n Final Win Rate vs Random:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Multiplier':<12} {'PUCT':<15} {'TTTS':<15}\")\n",
    "for mult in [1, 4, 8]:\n",
    "    puct_wr = puct_metrics[mult]['win_rates'][-1][1] if puct_metrics[mult]['win_rates'] else 0\n",
    "    ttts_wr = ttts_metrics[mult]['win_rates'][-1][1] if ttts_metrics[mult]['win_rates'] else 0\n",
    "    print(f\"{mult}x{'':<10} {puct_wr:<15.0%} {ttts_wr:<15.0%}\")\n",
    "\n",
    "print(\"\\n Conclusions:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"- Higher batch multipliers provide faster training\")\n",
    "print(\"- Model quality should be similar across batch sizes\")\n",
    "print(\"- PUCT and TTTS show different sensitivity to batching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training speed comparison\n",
    "ax1 = axes[0, 0]\n",
    "mults = [1, 4, 8]\n",
    "puct_times = [np.mean(puct_metrics[m]['self_play_times']) for m in mults]\n",
    "ttts_times = [np.mean(ttts_metrics[m]['self_play_times']) for m in mults]\n",
    "\n",
    "x = np.arange(len(mults))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, puct_times, width, label='PUCT', color='#3498DB')\n",
    "ax1.bar(x + width/2, ttts_times, width, label='TTTS', color='#E67E22')\n",
    "ax1.set_xlabel('leaves_per_batch Multiplier')\n",
    "ax1.set_ylabel('Self-play Time (s)')\n",
    "ax1.set_title('Training Speed by Batch Size')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f'{m}x' for m in mults])\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Final win rates\n",
    "ax2 = axes[0, 1]\n",
    "puct_wrs = [puct_metrics[m]['win_rates'][-1][1] if puct_metrics[m]['win_rates'] else 0 for m in mults]\n",
    "ttts_wrs = [ttts_metrics[m]['win_rates'][-1][1] if ttts_metrics[m]['win_rates'] else 0 for m in mults]\n",
    "\n",
    "ax2.bar(x - width/2, puct_wrs, width, label='PUCT', color='#3498DB')\n",
    "ax2.bar(x + width/2, ttts_wrs, width, label='TTTS', color='#E67E22')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('leaves_per_batch Multiplier')\n",
    "ax2.set_ylabel('Win Rate vs Random')\n",
    "ax2.set_title('Final Model Quality')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f'{m}x' for m in mults])\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Learning curves - PUCT\n",
    "ax3 = axes[1, 0]\n",
    "for mult in mults:\n",
    "    iters = [wr[0] for wr in puct_metrics[mult]['win_rates']]\n",
    "    wrs = [wr[1] for wr in puct_metrics[mult]['win_rates']]\n",
    "    ax3.plot(iters, wrs, 'o-', label=f'{mult}x', linewidth=2)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Win Rate vs Random')\n",
    "ax3.set_title('PUCT Learning Curves')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Learning curves - TTTS\n",
    "ax4 = axes[1, 1]\n",
    "for mult in mults:\n",
    "    iters = [wr[0] for wr in ttts_metrics[mult]['win_rates']]\n",
    "    wrs = [wr[1] for wr in ttts_metrics[mult]['win_rates']]\n",
    "    ax4.plot(iters, wrs, 'o-', label=f'{mult}x', linewidth=2)\n",
    "ax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Iteration')\n",
    "ax4.set_ylabel('Win Rate vs Random')\n",
    "ax4.set_title('TTTS Learning Curves')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}