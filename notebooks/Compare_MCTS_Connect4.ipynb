{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Algorithm Comparison: PUCT vs BayesianMCTS\n",
    "\n",
    "This notebook compares BatchedMCTS (PUCT) vs BayesianMCTS (Thompson Sampling + IDS) on Connect4.\n",
    "\n",
    "**Metrics compared:**\n",
    "- Training throughput (games/sec, examples/sec)\n",
    "- Inference speed (searches/sec)\n",
    "- Policy quality (win rate vs random)\n",
    "- Policy agreement between algorithms\n",
    "\n",
    "**Recommended:** Run on A100 GPU for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone nanozero repo\n",
    "!git clone https://github.com/calaldred/nanozero.git\n",
    "%cd nanozero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from nanozero.game import get_game\n",
    "from nanozero.model import AlphaZeroTransformer\n",
    "from nanozero.mcts import BatchedMCTS, MCTSConfig, sample_action\n",
    "from nanozero.bayesian_mcts import BayesianMCTS, BayesianMCTSConfig\n",
    "from nanozero.replay import ReplayBuffer\n",
    "from nanozero.config import get_model_config\n",
    "from nanozero.common import set_seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Game and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Connect4 game\n",
    "game = get_game('connect4')\n",
    "print(f\"Game: Connect4\")\n",
    "print(f\"Board: {game.config.board_height}x{game.config.board_width}\")\n",
    "print(f\"Actions: {game.config.action_size}\")\n",
    "\n",
    "# Create model (4 layers for Connect4)\n",
    "model_config = get_model_config(game.config, n_layer=4)\n",
    "model = AlphaZeroTransformer(model_config).to(device)\n",
    "print(f\"Model parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both MCTS variants\n",
    "NUM_SIMS = 100  # Simulations per search\n",
    "\n",
    "puct_config = MCTSConfig(num_simulations=NUM_SIMS)\n",
    "puct_mcts = BatchedMCTS(game, puct_config)\n",
    "\n",
    "bayesian_config = BayesianMCTSConfig(\n",
    "    num_simulations=NUM_SIMS,\n",
    "    early_stopping=True,\n",
    "    confidence_threshold=0.95,\n",
    ")\n",
    "bayesian_mcts = BayesianMCTS(game, bayesian_config)\n",
    "\n",
    "print(f\"PUCT MCTS: {NUM_SIMS} simulations\")\n",
    "print(f\"Bayesian MCTS: {NUM_SIMS} simulations (with early stopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Inference Speed\n",
    "\n",
    "Compare raw search speed for both algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def benchmark_inference(mcts, model, game, batch_sizes=[1, 8, 32, 64], num_searches=50):\n",
    "    \"\"\"Benchmark inference speed at various batch sizes.\"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Create batch of random states (a few moves in)\n",
    "        states = []\n",
    "        for _ in range(batch_size):\n",
    "            state = game.initial_state()\n",
    "            # Play 3-6 random moves\n",
    "            for _ in range(np.random.randint(3, 7)):\n",
    "                if game.is_terminal(state):\n",
    "                    break\n",
    "                legal = game.legal_actions(state)\n",
    "                state = game.next_state(state, np.random.choice(legal))\n",
    "            if game.is_terminal(state):\n",
    "                state = game.initial_state()\n",
    "            states.append(state)\n",
    "        \n",
    "        states = np.stack(states)\n",
    "        mcts.clear_cache()\n",
    "        \n",
    "        # Warmup\n",
    "        _ = mcts.search(states, model)\n",
    "        mcts.clear_cache()\n",
    "        \n",
    "        # Timed runs\n",
    "        times = []\n",
    "        for _ in range(num_searches):\n",
    "            mcts.clear_cache()\n",
    "            start = time.perf_counter()\n",
    "            _ = mcts.search(states, model)\n",
    "            torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "            times.append(time.perf_counter() - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        searches_per_sec = batch_size / avg_time\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'searches_per_sec': searches_per_sec,\n",
    "        }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmarking PUCT inference...\")\n",
    "puct_results = benchmark_inference(puct_mcts, model, game)\n",
    "\n",
    "print(\"Benchmarking Bayesian inference...\")\n",
    "bayesian_results = benchmark_inference(bayesian_mcts, model, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    INFERENCE SPEED COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Batch':<8} {'PUCT (s/s)':<15} {'Bayesian (s/s)':<15} {'Ratio':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for batch_size in puct_results.keys():\n",
    "    puct_sps = puct_results[batch_size]['searches_per_sec']\n",
    "    bay_sps = bayesian_results[batch_size]['searches_per_sec']\n",
    "    ratio = bay_sps / puct_sps\n",
    "    print(f\"{batch_size:<8} {puct_sps:<15.1f} {bay_sps:<15.1f} {ratio:<10.2f}x\")\n",
    "\n",
    "print(\"\\n(s/s = searches per second, higher is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Training Throughput\n",
    "\n",
    "Compare self-play speed (games per second, examples per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def self_play_games(game, model, mcts, num_games, parallel_games=32, \n",
    "                    temperature_threshold=15, mcts_type='puct'):\n",
    "    \"\"\"\n",
    "    Play self-play games and return examples + timing info.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_examples = []\n",
    "    games_completed = 0\n",
    "    total_searches = 0\n",
    "    \n",
    "    n_parallel = min(parallel_games, num_games)\n",
    "    states = [game.initial_state() for _ in range(n_parallel)]\n",
    "    move_counts = [0] * n_parallel\n",
    "    game_examples = [[] for _ in range(n_parallel)]\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    while games_completed < num_games:\n",
    "        active_indices = [i for i, s in enumerate(states) if not game.is_terminal(s)]\n",
    "        \n",
    "        if not active_indices:\n",
    "            break\n",
    "        \n",
    "        # Batch MCTS search\n",
    "        active_states = np.stack([states[i] for i in active_indices])\n",
    "        \n",
    "        if mcts_type == 'puct':\n",
    "            # Split by noise requirement\n",
    "            policies = np.zeros((len(active_indices), game.config.action_size), dtype=np.float32)\n",
    "            noise_indices = [i for i in range(len(active_indices)) if move_counts[active_indices[i]] == 0]\n",
    "            no_noise_indices = [i for i in range(len(active_indices)) if move_counts[active_indices[i]] != 0]\n",
    "            \n",
    "            if noise_indices:\n",
    "                noise_states = active_states[noise_indices]\n",
    "                noise_policies = mcts.search(noise_states, model, add_noise=True)\n",
    "                for local_idx, idx in enumerate(noise_indices):\n",
    "                    policies[idx] = noise_policies[local_idx]\n",
    "            \n",
    "            if no_noise_indices:\n",
    "                no_noise_states = active_states[no_noise_indices]\n",
    "                no_noise_policies = mcts.search(no_noise_states, model, add_noise=False)\n",
    "                for local_idx, idx in enumerate(no_noise_indices):\n",
    "                    policies[idx] = no_noise_policies[local_idx]\n",
    "        else:\n",
    "            policies = mcts.search(active_states, model)\n",
    "        \n",
    "        total_searches += len(active_indices)\n",
    "        \n",
    "        # Process moves\n",
    "        for idx, game_idx in enumerate(active_indices):\n",
    "            state = states[game_idx]\n",
    "            policy = policies[idx]\n",
    "            player = game.current_player(state)\n",
    "            move_count = move_counts[game_idx]\n",
    "            \n",
    "            canonical = game.canonical_state(state)\n",
    "            game_examples[game_idx].append((canonical.copy(), policy.copy(), player))\n",
    "            \n",
    "            temperature = 1.0 if move_count < temperature_threshold else 0.0\n",
    "            action = sample_action(policy, temperature=temperature)\n",
    "            \n",
    "            states[game_idx] = game.next_state(state, action)\n",
    "            move_counts[game_idx] += 1\n",
    "        \n",
    "        # Check finished games\n",
    "        for i in range(n_parallel):\n",
    "            if game.is_terminal(states[i]) and game_examples[i]:\n",
    "                reward = game.terminal_reward(states[i])\n",
    "                final_player = game.current_player(states[i])\n",
    "                \n",
    "                for canonical, policy, player in game_examples[i]:\n",
    "                    value = reward if player == final_player else -reward\n",
    "                    for sym_state, sym_policy in game.symmetries(canonical, policy):\n",
    "                        all_examples.append((sym_state, sym_policy, value))\n",
    "                \n",
    "                games_completed += 1\n",
    "                \n",
    "                if games_completed < num_games:\n",
    "                    states[i] = game.initial_state()\n",
    "                    move_counts[i] = 0\n",
    "                    game_examples[i] = []\n",
    "    \n",
    "    elapsed = time.perf_counter() - start_time\n",
    "    \n",
    "    return {\n",
    "        'examples': all_examples,\n",
    "        'games': games_completed,\n",
    "        'searches': total_searches,\n",
    "        'elapsed': elapsed,\n",
    "        'games_per_sec': games_completed / elapsed,\n",
    "        'examples_per_sec': len(all_examples) / elapsed,\n",
    "        'searches_per_sec': total_searches / elapsed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 50\n",
    "PARALLEL_GAMES = 32\n",
    "\n",
    "print(f\"Running self-play benchmark: {NUM_GAMES} games, {PARALLEL_GAMES} parallel\\n\")\n",
    "\n",
    "# Clear caches\n",
    "puct_mcts.clear_cache()\n",
    "bayesian_mcts.clear_cache()\n",
    "\n",
    "print(\"PUCT self-play...\")\n",
    "puct_selfplay = self_play_games(game, model, puct_mcts, NUM_GAMES, PARALLEL_GAMES, mcts_type='puct')\n",
    "\n",
    "print(\"Bayesian self-play...\")\n",
    "bayesian_selfplay = self_play_games(game, model, bayesian_mcts, NUM_GAMES, PARALLEL_GAMES, mcts_type='bayesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                   SELF-PLAY THROUGHPUT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'PUCT':<20} {'Bayesian':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"{'Games completed':<25} {puct_selfplay['games']:<20} {bayesian_selfplay['games']:<20}\")\n",
    "print(f\"{'Total time (s)':<25} {puct_selfplay['elapsed']:<20.2f} {bayesian_selfplay['elapsed']:<20.2f}\")\n",
    "print(f\"{'Games/sec':<25} {puct_selfplay['games_per_sec']:<20.2f} {bayesian_selfplay['games_per_sec']:<20.2f}\")\n",
    "print(f\"{'Examples generated':<25} {len(puct_selfplay['examples']):<20} {len(bayesian_selfplay['examples']):<20}\")\n",
    "print(f\"{'Examples/sec':<25} {puct_selfplay['examples_per_sec']:<20.1f} {bayesian_selfplay['examples_per_sec']:<20.1f}\")\n",
    "print(f\"{'Searches/sec':<25} {puct_selfplay['searches_per_sec']:<20.1f} {bayesian_selfplay['searches_per_sec']:<20.1f}\")\n",
    "\n",
    "speedup = puct_selfplay['games_per_sec'] / bayesian_selfplay['games_per_sec']\n",
    "print(f\"\\nPUCT is {speedup:.2f}x {'faster' if speedup > 1 else 'slower'} than Bayesian for self-play\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 3: Training Loop Comparison\n",
    "\n",
    "Train both algorithms for a few iterations and compare learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, scaler, batch, game, device, use_amp=True):\n",
    "    \"\"\"Single training step with mixed precision.\"\"\"\n",
    "    states, policies, values = batch\n",
    "    \n",
    "    state_tensors = torch.stack([game.to_tensor(s) for s in states]).to(device)\n",
    "    policy_tensors = torch.from_numpy(policies).float().to(device)\n",
    "    value_tensors = torch.from_numpy(values).float().to(device)\n",
    "    action_masks = torch.stack([\n",
    "        torch.from_numpy(game.legal_actions_mask(s)) for s in states\n",
    "    ]).float().to(device)\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "        pred_log_policies, pred_values = model(state_tensors, action_masks)\n",
    "        policy_loss = -torch.mean(torch.sum(policy_tensors * pred_log_policies, dim=1))\n",
    "        value_loss = F.mse_loss(pred_values.squeeze(-1), value_tensors)\n",
    "        loss = policy_loss + value_loss\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item(), policy_loss.item(), value_loss.item()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_vs_random(game, model, mcts, num_games=30, mcts_type='puct'):\n",
    "    \"\"\"Evaluate against random player.\"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        state = game.initial_state()\n",
    "        model_player = 1 if i % 2 == 0 else -1\n",
    "        \n",
    "        while not game.is_terminal(state):\n",
    "            current = game.current_player(state)\n",
    "            \n",
    "            if current == model_player:\n",
    "                if mcts_type == 'puct':\n",
    "                    policy = mcts.search(state[np.newaxis, ...], model, \n",
    "                                        num_simulations=50, add_noise=False)[0]\n",
    "                else:\n",
    "                    policy = mcts.search(state[np.newaxis, ...], model, \n",
    "                                        num_simulations=50)[0]\n",
    "                action = sample_action(policy, temperature=0)\n",
    "            else:\n",
    "                legal = game.legal_actions(state)\n",
    "                action = np.random.choice(legal)\n",
    "            \n",
    "            state = game.next_state(state, action)\n",
    "        \n",
    "        reward = game.terminal_reward(state)\n",
    "        final_player = game.current_player(state)\n",
    "        model_result = reward if final_player == model_player else -reward\n",
    "        \n",
    "        if model_result > 0:\n",
    "            wins += 1\n",
    "    \n",
    "    return wins / num_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_ITERATIONS = 10\n",
    "GAMES_PER_ITER = 30\n",
    "TRAINING_STEPS = 100\n",
    "BATCH_SIZE = 64\n",
    "PARALLEL_GAMES = 32\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"  Games/iteration: {GAMES_PER_ITER}\")\n",
    "print(f\"  Training steps/iteration: {TRAINING_STEPS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(game, model, mcts, mcts_type, num_iterations, games_per_iter, \n",
    "                 training_steps, batch_size, parallel_games, device):\n",
    "    \"\"\"Run training loop and collect metrics.\"\"\"\n",
    "    \n",
    "    # Fresh model\n",
    "    model_config = get_model_config(game.config, n_layer=4)\n",
    "    model = AlphaZeroTransformer(model_config).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))\n",
    "    use_amp = device.type == 'cuda'\n",
    "    \n",
    "    buffer = ReplayBuffer(50000)\n",
    "    \n",
    "    metrics = {\n",
    "        'iteration': [],\n",
    "        'selfplay_time': [],\n",
    "        'train_time': [],\n",
    "        'loss': [],\n",
    "        'win_rate': [],\n",
    "        'examples': [],\n",
    "    }\n",
    "    \n",
    "    total_start = time.perf_counter()\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"  Iteration {iteration + 1}/{num_iterations}\", end=\" \")\n",
    "        \n",
    "        # Self-play\n",
    "        mcts.clear_cache()\n",
    "        sp_start = time.perf_counter()\n",
    "        result = self_play_games(game, model, mcts, games_per_iter, parallel_games, mcts_type=mcts_type)\n",
    "        sp_time = time.perf_counter() - sp_start\n",
    "        \n",
    "        for state, policy, value in result['examples']:\n",
    "            buffer.push(state, policy, value)\n",
    "        \n",
    "        # Training\n",
    "        train_start = time.perf_counter()\n",
    "        total_loss = 0\n",
    "        \n",
    "        if len(buffer) >= batch_size:\n",
    "            for _ in range(training_steps):\n",
    "                batch = buffer.sample(batch_size)\n",
    "                loss, _, _ = train_step(model, optimizer, scaler, batch, game, device, use_amp)\n",
    "                total_loss += loss\n",
    "        \n",
    "        train_time = time.perf_counter() - train_start\n",
    "        avg_loss = total_loss / training_steps if training_steps > 0 else 0\n",
    "        \n",
    "        # Evaluate every 2 iterations\n",
    "        win_rate = 0\n",
    "        if (iteration + 1) % 2 == 0:\n",
    "            mcts.clear_cache()\n",
    "            win_rate = evaluate_vs_random(game, model, mcts, num_games=20, mcts_type=mcts_type)\n",
    "        \n",
    "        metrics['iteration'].append(iteration + 1)\n",
    "        metrics['selfplay_time'].append(sp_time)\n",
    "        metrics['train_time'].append(train_time)\n",
    "        metrics['loss'].append(avg_loss)\n",
    "        metrics['win_rate'].append(win_rate)\n",
    "        metrics['examples'].append(len(result['examples']))\n",
    "        \n",
    "        print(f\"| SP: {sp_time:.1f}s | Train: {train_time:.1f}s | Loss: {avg_loss:.3f} | WR: {win_rate:.0%}\")\n",
    "    \n",
    "    total_time = time.perf_counter() - total_start\n",
    "    metrics['total_time'] = total_time\n",
    "    metrics['model'] = model\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Training with PUCT MCTS\")\n",
    "print(\"=\"*70)\n",
    "puct_metrics = run_training(\n",
    "    game, model, puct_mcts, 'puct',\n",
    "    NUM_ITERATIONS, GAMES_PER_ITER, TRAINING_STEPS, BATCH_SIZE, PARALLEL_GAMES, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Training with Bayesian MCTS\")\n",
    "print(\"=\"*70)\n",
    "bayesian_metrics = run_training(\n",
    "    game, model, bayesian_mcts, 'bayesian',\n",
    "    NUM_ITERATIONS, GAMES_PER_ITER, TRAINING_STEPS, BATCH_SIZE, PARALLEL_GAMES, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    TRAINING COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'PUCT':<20} {'Bayesian':<20}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(f\"{'Total training time (s)':<30} {puct_metrics['total_time']:<20.1f} {bayesian_metrics['total_time']:<20.1f}\")\n",
    "print(f\"{'Avg self-play time/iter (s)':<30} {np.mean(puct_metrics['selfplay_time']):<20.2f} {np.mean(bayesian_metrics['selfplay_time']):<20.2f}\")\n",
    "print(f\"{'Avg training time/iter (s)':<30} {np.mean(puct_metrics['train_time']):<20.2f} {np.mean(bayesian_metrics['train_time']):<20.2f}\")\n",
    "print(f\"{'Final loss':<30} {puct_metrics['loss'][-1]:<20.3f} {bayesian_metrics['loss'][-1]:<20.3f}\")\n",
    "print(f\"{'Final win rate vs random':<30} {puct_metrics['win_rate'][-1]:<20.0%} {bayesian_metrics['win_rate'][-1]:<20.0%}\")\n",
    "print(f\"{'Total examples generated':<30} {sum(puct_metrics['examples']):<20} {sum(bayesian_metrics['examples']):<20}\")\n",
    "\n",
    "speedup = puct_metrics['total_time'] / bayesian_metrics['total_time']\n",
    "print(f\"\\nBayesian is {speedup:.2f}x {'faster' if speedup > 1 else 'slower'} than PUCT overall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 4: Policy Agreement Analysis\n",
    "\n",
    "Compare policies produced by both algorithms on the same positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs, eps=1e-8):\n",
    "    \"\"\"Shannon entropy of probability distribution.\"\"\"\n",
    "    probs = probs + eps\n",
    "    probs = probs / probs.sum()\n",
    "    return float(-np.sum(probs * np.log(probs)))\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def compare_policies(game, model, puct_mcts, bayesian_mcts, num_positions=50, num_sims=100):\n",
    "    \"\"\"Compare policies from both algorithms on same positions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    results = {\n",
    "        'agree': 0,\n",
    "        'puct_entropy': [],\n",
    "        'bayesian_entropy': [],\n",
    "        'puct_confidence': [],\n",
    "        'bayesian_confidence': [],\n",
    "    }\n",
    "    \n",
    "    for _ in range(num_positions):\n",
    "        # Generate random position\n",
    "        state = game.initial_state()\n",
    "        for _ in range(np.random.randint(2, 10)):\n",
    "            if game.is_terminal(state):\n",
    "                break\n",
    "            legal = game.legal_actions(state)\n",
    "            state = game.next_state(state, np.random.choice(legal))\n",
    "        \n",
    "        if game.is_terminal(state):\n",
    "            state = game.initial_state()\n",
    "        \n",
    "        # Get policies\n",
    "        puct_mcts.clear_cache()\n",
    "        bayesian_mcts.clear_cache()\n",
    "        \n",
    "        puct_policy = puct_mcts.search(state[np.newaxis, ...], model, \n",
    "                                       num_simulations=num_sims, add_noise=False)[0]\n",
    "        bayesian_policy = bayesian_mcts.search(state[np.newaxis, ...], model,\n",
    "                                               num_simulations=num_sims)[0]\n",
    "        \n",
    "        # Compare\n",
    "        puct_best = np.argmax(puct_policy)\n",
    "        bayesian_best = np.argmax(bayesian_policy)\n",
    "        \n",
    "        if puct_best == bayesian_best:\n",
    "            results['agree'] += 1\n",
    "        \n",
    "        results['puct_entropy'].append(entropy(puct_policy))\n",
    "        results['bayesian_entropy'].append(entropy(bayesian_policy))\n",
    "        results['puct_confidence'].append(float(np.max(puct_policy)))\n",
    "        results['bayesian_confidence'].append(float(np.max(bayesian_policy)))\n",
    "    \n",
    "    results['agreement_rate'] = results['agree'] / num_positions\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained PUCT model for comparison\n",
    "trained_model = puct_metrics['model']\n",
    "\n",
    "print(\"Comparing policies on 50 random positions...\")\n",
    "policy_comparison = compare_policies(game, trained_model, puct_mcts, bayesian_mcts, \n",
    "                                     num_positions=50, num_sims=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                      POLICY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nAgreement rate (same best action): {policy_comparison['agreement_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'PUCT':<15} {'Bayesian':<15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'Avg entropy':<25} {np.mean(policy_comparison['puct_entropy']):<15.3f} {np.mean(policy_comparison['bayesian_entropy']):<15.3f}\")\n",
    "print(f\"{'Avg confidence (max prob)':<25} {np.mean(policy_comparison['puct_confidence']):<15.3f} {np.mean(policy_comparison['bayesian_confidence']):<15.3f}\")\n",
    "\n",
    "print(\"\\n(Lower entropy = more confident, higher confidence = stronger preference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                         FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. INFERENCE SPEED (searches/sec at batch=32):\")\n",
    "print(f\"   PUCT:     {puct_results[32]['searches_per_sec']:.1f}\")\n",
    "print(f\"   Bayesian: {bayesian_results[32]['searches_per_sec']:.1f}\")\n",
    "\n",
    "print(f\"\\n2. TRAINING THROUGHPUT:\")\n",
    "print(f\"   PUCT:     {puct_metrics['total_time']:.1f}s total\")\n",
    "print(f\"   Bayesian: {bayesian_metrics['total_time']:.1f}s total\")\n",
    "\n",
    "print(f\"\\n3. LEARNING (win rate vs random after {NUM_ITERATIONS} iterations):\")\n",
    "print(f\"   PUCT:     {puct_metrics['win_rate'][-1]:.0%}\")\n",
    "print(f\"   Bayesian: {bayesian_metrics['win_rate'][-1]:.0%}\")\n",
    "\n",
    "print(f\"\\n4. POLICY AGREEMENT: {policy_comparison['agreement_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
